{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcq4zVeM_6oY"
      },
      "source": [
        "# 🚀 How to Run This Notebook in Google Colab\n",
        "\n",
        "Follow these steps to run this notebook in Google Colab:\n",
        "\n",
        "1. **Upload the notebook to Google Colab**:\n",
        "   - Download this notebook file\n",
        "   - Go to [Google Colab](https://colab.research.google.com/)\n",
        "   - Click `File > Upload notebook` and select the downloaded file\n",
        "\n",
        "2. **Mount Google Drive**:\n",
        "   - Execute the first code cell to mount your Google Drive\n",
        "   - Follow the authentication instructions to grant Colab access to your Drive\n",
        "\n",
        "3. **Set up Kaggle API**:\n",
        "   - Go to [your Kaggle account](https://www.kaggle.com/account)\n",
        "   - Click on \"Create New API Token\" to download `kaggle.json`\n",
        "   - In Colab, click the folder icon on the left sidebar, then upload the `kaggle.json` file\n",
        "   - Run the second code cell to set up the Kaggle API and download the dataset\n",
        "\n",
        "4. **Run the Code**:\n",
        "   - Execute all cells in order\n",
        "   - When ready to train, execute the `main_colab()` function instead of the original `main()` function\n",
        "   \n",
        "5. **Check Your Results**:\n",
        "   - Once training is complete, your model checkpoints and outputs will be saved to:\n",
        "     - Google Drive: `/content/drive/MyDrive/Contour_Mamba_Models/`\n",
        "     - Local Colab: `/content/polygen_output/`\n",
        "\n",
        "6. **Accessing Your Model After the Session Ends**:\n",
        "   - Your model will be safe in Google Drive even after the Colab session expires\n",
        "   - Find it at: `MyDrive/Contour_Mamba_Models/checkpoints/best_model_polygen.pth`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEWA0ger_6oS"
      },
      "source": [
        "# Setup for Google Colab: Mount Drive and Connect to Kaggle Dataset\n",
        "\n",
        "This notebook contains code to:\n",
        "1. Mount Google Drive to save model checkpoints\n",
        "2. Connect to and download the PolyGen2021 Kaggle dataset\n",
        "3. Configure paths to save the best model to your Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxbiPKn4_6oU",
        "outputId": "fe547e86-152e-4b55-b94e-ba46dcc9139d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model checkpoint directory created at: /content/drive/MyDrive/Contour_Mamba_Models\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive to save your model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a directory for saving models in Google Drive\n",
        "import os\n",
        "DRIVE_MODEL_DIR = \"/content/drive/MyDrive/Contour_Mamba_Models\"\n",
        "os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
        "print(f\"Model checkpoint directory created at: {DRIVE_MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9HeOPtF_6oV",
        "outputId": "8139ff2c-5ca6-4856-eab0-205b60f4e921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kokoroou/polypgen2021-segmentation-2\n",
            "License(s): unknown\n",
            "Downloading polypgen2021-segmentation-2.zip to /content\n",
            " 91% 999M/1.07G [00:00<00:00, 1.05GB/s]\n",
            "100% 1.07G/1.07G [00:01<00:00, 1.10GB/s]\n",
            "total 12\n",
            "drwxr-xr-x 3 root root 4096 Jun 14 05:58 .\n",
            "drwxr-xr-x 1 root root 4096 Jun 14 05:58 ..\n",
            "drwxr-xr-x 4 root root 4096 Jun 14 05:58 PolypGen2021_segmentation\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Connect to Kaggle API and download the dataset\n",
        "# You'll need to upload your Kaggle API token to Colab\n",
        "# Follow these steps:\n",
        "# 1. Go to kaggle.com → Account → Create API Token → download kaggle.json\n",
        "# 2. Upload this file to Colab using the file browser\n",
        "\n",
        "# Run this after uploading your kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the PolyGen2021 dataset\n",
        "# URL: https://www.kaggle.com/datasets/kokoroou/polypgen2021-segmentation-2\n",
        "!kaggle datasets download -d kokoroou/polypgen2021-segmentation-2\n",
        "\n",
        "# Extract the dataset\n",
        "import zipfile\n",
        "!mkdir -p /content/polypgen2021\n",
        "!unzip -q polypgen2021-segmentation-2.zip -d /content/polypgen2021\n",
        "\n",
        "# Verify the dataset structure\n",
        "!ls -la /content/polypgen2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D_ORw7zv_6oW"
      },
      "outputs": [],
      "source": [
        "# Step 3: Modify the paths for Google Colab and add functionality to save to Drive\n",
        "# This function updates the main() function to use Google Drive for checkpoints\n",
        "\n",
        "def update_paths_for_colab():\n",
        "    \"\"\"\n",
        "    Update the global paths to work with Google Colab and Google Drive\n",
        "    Run this before executing the main() function\n",
        "    \"\"\"\n",
        "    global OUTPUT_DIR, CHECKPOINT_DIR, POLYGEN_DATASET_PATH\n",
        "\n",
        "    # Set paths for Google Colab\n",
        "    OUTPUT_DIR = \"/content/polygen_output\"\n",
        "    CHECKPOINT_DIR = \"/content/drive/MyDrive/Contour_Mamba_Models/checkpoints\"\n",
        "    POLYGEN_DATASET_PATH = \"/content/polypgen2021\"\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    print(f\"Output directory set to: {OUTPUT_DIR}\")\n",
        "    print(f\"Checkpoint directory set to: {CHECKPOINT_DIR}\")\n",
        "    print(f\"Dataset path set to: {POLYGEN_DATASET_PATH}\")\n",
        "\n",
        "    return OUTPUT_DIR, CHECKPOINT_DIR, POLYGEN_DATASET_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-06-12T12:34:08.856996Z",
          "iopub.status.busy": "2025-06-12T12:34:08.856682Z",
          "iopub.status.idle": "2025-06-12T12:34:08.931217Z",
          "shell.execute_reply": "2025-06-12T12:34:08.930467Z",
          "shell.execute_reply.started": "2025-06-12T12:34:08.856977Z"
        },
        "trusted": true,
        "id": "E1YH_IZb_6oZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from einops import rearrange\n",
        "import math\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.amp import GradScaler, autocast\n",
        "\n",
        "# -------------------------\n",
        "# Multi-Kernel Positional Embedding Module\n",
        "# -------------------------\n",
        "class MultiKernelPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=8):\n",
        "        super(MultiKernelPositionalEmbedding, self).__init__()\n",
        "        self.mid_channels = max(8, in_channels // reduction)\n",
        "\n",
        "        self.conv3x3 = nn.Conv2d(in_channels, self.mid_channels, kernel_size=3, padding=1)\n",
        "        self.conv5x5 = nn.Conv2d(in_channels, self.mid_channels, kernel_size=5, padding=2)\n",
        "        self.conv7x7 = nn.Conv2d(in_channels, self.mid_channels, kernel_size=7, padding=3)\n",
        "\n",
        "        self.position_attention = nn.Sequential(\n",
        "            nn.Conv2d(self.mid_channels * 3, in_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat_3x3 = self.conv3x3(x)\n",
        "        feat_5x5 = self.conv5x5(x)\n",
        "        feat_7x7 = self.conv7x7(x)\n",
        "\n",
        "        multi_scale_feat = torch.cat([feat_3x3, feat_5x5, feat_7x7], dim=1)\n",
        "        attention_map = self.position_attention(multi_scale_feat)\n",
        "        enhanced = x * attention_map\n",
        "        return enhanced\n",
        "\n",
        "# -------------------------\n",
        "# Learnable Contour Extractor\n",
        "# -------------------------\n",
        "class LearnableContourExtractor(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(LearnableContourExtractor, self).__init__()\n",
        "\n",
        "        self.sobel_x = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False, groups=in_channels)\n",
        "        self.sobel_y = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False, groups=in_channels)\n",
        "\n",
        "        sobel_x_kernel = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
        "        sobel_y_kernel = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(in_channels):\n",
        "                self.sobel_x.weight[i, 0] = sobel_x_kernel\n",
        "                self.sobel_y.weight[i, 0] = sobel_y_kernel\n",
        "\n",
        "        self.edge_enhance = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.contour_refine = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        grad_x = self.sobel_x(x)\n",
        "        grad_y = self.sobel_y(x)\n",
        "        gradient_magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)\n",
        "        enhanced_edges = self.edge_enhance(gradient_magnitude)\n",
        "        contour_features = self.contour_refine(enhanced_edges)\n",
        "        return contour_features\n",
        "\n",
        "# -------------------------\n",
        "# Contour-Guided Selective Scan\n",
        "# -------------------------\n",
        "def contour_guided_selective_scan(u, delta, A, B, C, D, contour_features):\n",
        "    try:\n",
        "        batch_size, L, D_in = u.shape\n",
        "        N = A.shape[-1]\n",
        "\n",
        "        A = torch.clamp(A, min=-8.0, max=0.1)\n",
        "        delta = torch.clamp(delta, min=1e-8, max=10.0)\n",
        "\n",
        "        if contour_features.dim() == 4:\n",
        "            b, c, h, w = contour_features.shape\n",
        "            contour_features = contour_features.flatten(2).transpose(1, 2)\n",
        "            if contour_features.shape[1] != L:\n",
        "                contour_features = F.interpolate(\n",
        "                    contour_features.transpose(1, 2).view(b, c, h, w),\n",
        "                    size=(int(L**0.5), int(L**0.5)),\n",
        "                    mode='bilinear',\n",
        "                    align_corners=False\n",
        "                ).flatten(2).transpose(1, 2)\n",
        "            if contour_features.shape[2] != D_in:\n",
        "                contour_features = F.adaptive_avg_pool1d(\n",
        "                    contour_features.transpose(1, 2), D_in\n",
        "                ).transpose(1, 2)\n",
        "\n",
        "        contour_weight = torch.sigmoid(contour_features)\n",
        "        delta_modulated = delta * (1.0 + 0.2 * contour_weight)\n",
        "\n",
        "        chunk_size = min(64, L)\n",
        "        outputs = []\n",
        "        h_state = torch.zeros(batch_size, D_in, N, device=u.device, dtype=u.dtype)\n",
        "\n",
        "        for i in range(0, L, chunk_size):\n",
        "            end_idx = min(i + chunk_size, L)\n",
        "            chunk_len = end_idx - i\n",
        "\n",
        "            u_chunk = u[:, i:end_idx]\n",
        "            delta_chunk = delta_modulated[:, i:end_idx]\n",
        "            B_chunk = B[:, i:end_idx]\n",
        "            C_chunk = C[:, i:end_idx]\n",
        "\n",
        "            dA = torch.einsum('bld,dn->bldn', delta_chunk, A)\n",
        "            dB_u = torch.einsum('bld,bld,bln->bldn', delta_chunk, u_chunk, B_chunk)\n",
        "\n",
        "            dA_exp = torch.exp(torch.clamp(dA, min=-10.0, max=10.0))\n",
        "            h_chunk = torch.zeros_like(dB_u)\n",
        "\n",
        "            for t in range(chunk_len):\n",
        "                h_state = h_state * dA_exp[:, t] + dB_u[:, t]\n",
        "                h_chunk[:, t] = h_state\n",
        "\n",
        "            y_chunk = torch.einsum('bldn,bln->bld', h_chunk, C_chunk)\n",
        "            outputs.append(y_chunk)\n",
        "\n",
        "        y = torch.cat(outputs, dim=1)\n",
        "        return y + u * D.view(1, 1, -1).expand(batch_size, L, D_in)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in contour_guided_selective_scan: {e}\")\n",
        "        print(f\"u shape: {u.shape}, delta shape: {delta.shape}\")\n",
        "        print(f\"A shape: {A.shape}, B shape: {B.shape}, C shape: {C.shape}\")\n",
        "        print(f\"contour_features shape: {contour_features.shape}\")\n",
        "        raise\n",
        "\n",
        "# -------------------------\n",
        "# Channel Attention Module\n",
        "# -------------------------\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_channels // reduction, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, l, c = x.shape\n",
        "        y = self.avg_pool(x.transpose(1, 2)).view(b, c)\n",
        "        y = self.fc(y).view(b, 1, c)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "# -------------------------\n",
        "# Contour-Aware Mamba Block\n",
        "# -------------------------\n",
        "class ContourAwareMambaBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_state=32, expand_factor=1.5):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_inner = int(expand_factor * d_model)\n",
        "\n",
        "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
        "        self.conv1d = nn.Conv1d(self.d_inner, self.d_inner, kernel_size=3, padding=1, groups=self.d_inner)\n",
        "        self.x_proj = nn.Linear(self.d_inner, d_state * 2 + 16, bias=False)\n",
        "        self.delta_proj = nn.Linear(16, self.d_inner, bias=True)\n",
        "\n",
        "        self.contour_extractor = LearnableContourExtractor(d_model)\n",
        "        self.contour_proj = nn.Linear(d_model, self.d_inner, bias=False)\n",
        "        self.channel_attention = ChannelAttention(self.d_inner)\n",
        "\n",
        "        A = torch.arange(1, d_state + 1, dtype=torch.float32).unsqueeze(0).repeat(self.d_inner, 1)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
        "\n",
        "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.norm_contour = nn.LayerNorm(self.d_inner)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        nn.init.xavier_uniform_(self.in_proj.weight, gain=0.5)\n",
        "        nn.init.xavier_uniform_(self.x_proj.weight, gain=0.5)\n",
        "        nn.init.xavier_uniform_(self.delta_proj.weight, gain=0.5)\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight, gain=0.5)\n",
        "        nn.init.xavier_uniform_(self.contour_proj.weight, gain=0.5)\n",
        "        nn.init.constant_(self.delta_proj.bias, 1.0)\n",
        "\n",
        "    def forward(self, x, spatial_dims=None):\n",
        "        b, l, d = x.shape\n",
        "\n",
        "        if spatial_dims is None:\n",
        "            h = w = int(l ** 0.5)\n",
        "            if h * w != l:\n",
        "                h = int((l / 4) ** 0.5) * 2\n",
        "                w = l // h\n",
        "        else:\n",
        "            h, w = spatial_dims\n",
        "\n",
        "        x_spatial = x.transpose(1, 2).reshape(b, d, h, w)\n",
        "        contour_spatial = self.contour_extractor(x_spatial)\n",
        "        contour_flat = contour_spatial.flatten(2).transpose(1, 2)\n",
        "        contour_features = self.contour_proj(contour_flat)\n",
        "        contour_features = self.norm_contour(contour_features)\n",
        "\n",
        "        x_and_res = self.in_proj(x)\n",
        "        x_ssm, res = x_and_res.chunk(2, dim=-1)\n",
        "\n",
        "        x_ssm = rearrange(x_ssm, 'b l d -> b d l')\n",
        "        x_ssm = self.conv1d(x_ssm)\n",
        "        x_ssm = rearrange(x_ssm, 'b d l -> b l d')\n",
        "        x_ssm = F.silu(x_ssm)\n",
        "\n",
        "        A = -torch.exp(self.A_log)\n",
        "        x_dbl = self.x_proj(x_ssm)\n",
        "        delta, B, C = torch.split(x_dbl, [16, self.d_state, self.d_state], dim=-1)\n",
        "        delta = F.softplus(self.delta_proj(delta))\n",
        "\n",
        "        y = contour_guided_selective_scan(x_ssm, delta, A, B, C, self.D, contour_features)\n",
        "        y = self.channel_attention(y)\n",
        "        y = y * F.silu(res)\n",
        "        y = self.out_proj(y)\n",
        "        return self.dropout(y)\n",
        "\n",
        "# -------------------------\n",
        "# Enhanced Residual Block\n",
        "# -------------------------\n",
        "class ContourAwareResidualBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_state=32, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.mixer = ContourAwareMambaBlock(d_model, d_state)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, spatial_dims=None):\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.mixer(x, spatial_dims)\n",
        "        x = self.dropout(x)\n",
        "        x = residual + x\n",
        "        return self.norm2(x)\n",
        "\n",
        "# -------------------------\n",
        "# Double Convolution with MKPE\n",
        "# -------------------------\n",
        "class DoubleConvWithMKPE(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.same_channels = in_channels == out_channels\n",
        "\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.mkpe = MultiKernelPositionalEmbedding(out_channels)\n",
        "\n",
        "        if not self.same_channels:\n",
        "            self.project = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x if self.same_channels else self.project(x)\n",
        "        x = self.double_conv(x)\n",
        "        x = self.mkpe(x)\n",
        "        x = x + identity\n",
        "        return x\n",
        "\n",
        "# -------------------------\n",
        "# MKPE + Contour-Aware Mamba UNet\n",
        "# -------------------------\n",
        "class MKPEContourAwareMambaUNet(nn.Module):\n",
        "    def __init__(self, num_classes=2, d_model=128, d_state=32, num_mamba_layers=6):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Encoder path\n",
        "        self.encoder1 = DoubleConvWithMKPE(3, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.encoder2 = DoubleConvWithMKPE(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.encoder3 = DoubleConvWithMKPE(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Mamba blocks\n",
        "        self.mamba_blocks = nn.ModuleList([\n",
        "            ContourAwareResidualBlock(d_model, d_state)\n",
        "            for _ in range(num_mamba_layers)\n",
        "        ])\n",
        "\n",
        "        # Bridge layers\n",
        "        self.bridge_down = nn.Conv2d(256, d_model, kernel_size=1)\n",
        "        self.bridge_up = nn.Conv2d(d_model, 256, kernel_size=1)\n",
        "        self.bottleneck_mkpe = MultiKernelPositionalEmbedding(256)\n",
        "\n",
        "        # Decoder path\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder3 = DoubleConvWithMKPE(256, 128)\n",
        "        self.deep_sup3 = nn.Conv2d(128, num_classes, kernel_size=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder2 = DoubleConvWithMKPE(128, 64)\n",
        "        self.deep_sup2 = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.decoder1 = DoubleConvWithMKPE(35, 32)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=1)\n",
        "        self.output_mkpe = MultiKernelPositionalEmbedding(num_classes, reduction=2)\n",
        "\n",
        "    def forward(self, x, return_deep=False):\n",
        "        input_x = x\n",
        "\n",
        "        # Encoder\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc1_pool = self.pool1(enc1)\n",
        "        enc2 = self.encoder2(enc1_pool)\n",
        "        enc2_pool = self.pool2(enc2)\n",
        "        enc3 = self.encoder3(enc2_pool)\n",
        "        enc3_pool = self.pool3(enc3)\n",
        "\n",
        "        # Bridge with Mamba processing\n",
        "        bridge_out = self.bridge_down(enc3_pool)\n",
        "        b, c, h, w = bridge_out.size()\n",
        "        mamba_input = bridge_out.permute(0, 2, 3, 1).reshape(b, h * w, c)\n",
        "\n",
        "        mamba_output = mamba_input\n",
        "        for mamba_block in self.mamba_blocks:\n",
        "            mamba_output = mamba_block(mamba_output, spatial_dims=(h, w))\n",
        "\n",
        "        mamba_output = mamba_output.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
        "        mamba_output = self.bridge_up(mamba_output)\n",
        "        mamba_output = self.bottleneck_mkpe(mamba_output)\n",
        "\n",
        "        # Decoder path\n",
        "        dec3 = self.upconv3(mamba_output)\n",
        "        if dec3.shape[2:] != enc2.shape[2:]:\n",
        "            dec3 = F.interpolate(dec3, size=enc2.shape[2:], mode='bilinear', align_corners=True)\n",
        "        dec3 = torch.cat([dec3, enc2], dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        deep_out3 = self.deep_sup3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        if dec2.shape[2:] != enc1.shape[2:]:\n",
        "            dec2 = F.interpolate(dec2, size=enc1.shape[2:], mode='bilinear', align_corners=True)\n",
        "        dec2 = torch.cat([dec2, enc1], dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        deep_out2 = self.deep_sup2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        if dec1.shape[2:] != input_x.shape[2:]:\n",
        "            dec1 = F.interpolate(dec1, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n",
        "        dec1 = torch.cat([dec1, input_x], dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        # Final output\n",
        "        out = self.final_conv(dec1)\n",
        "        out = self.output_mkpe(out)\n",
        "\n",
        "        if return_deep:\n",
        "            deep_out2 = F.interpolate(deep_out2, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n",
        "            deep_out3 = F.interpolate(deep_out3, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n",
        "            return out, deep_out2, deep_out3\n",
        "        return out\n",
        "\n",
        "# -------------------------\n",
        "# Combined Loss Function\n",
        "# -------------------------\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
        "        super().__init__()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "        self.bce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce = self.bce_loss(inputs, targets)\n",
        "        inputs_soft = F.softmax(inputs, dim=1)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n",
        "        intersection = (inputs_soft * targets_one_hot).sum(dim=(2, 3))\n",
        "        cardinality = inputs_soft.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))\n",
        "        dice = (2. * intersection / (cardinality + 1e-6)).mean()\n",
        "        dice_loss = 1 - dice\n",
        "        return self.bce_weight * bce + self.dice_weight * dice_loss\n",
        "\n",
        "# -------------------------\n",
        "# Deep Supervision Loss\n",
        "# -------------------------\n",
        "class DeepSupervisionLoss(nn.Module):\n",
        "    def __init__(self, main_weight=0.8, deep2_weight=0.1, deep3_weight=0.1):\n",
        "        super().__init__()\n",
        "        self.main_weight = main_weight\n",
        "        self.deep2_weight = deep2_weight\n",
        "        self.deep3_weight = deep3_weight\n",
        "        self.criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n",
        "\n",
        "    def forward(self, outputs, target):\n",
        "        main_out, deep2, deep3 = outputs\n",
        "        loss_main = self.criterion(main_out, target)\n",
        "        loss_deep2 = self.criterion(deep2, target)\n",
        "        loss_deep3 = self.criterion(deep3, target)\n",
        "        total_loss = (\n",
        "            self.main_weight * loss_main +\n",
        "            self.deep2_weight * loss_deep2 +\n",
        "            self.deep3_weight * loss_deep3\n",
        "        )\n",
        "        return total_loss\n",
        "\n",
        "# -------------------------\n",
        "# Evaluation Metrics\n",
        "# -------------------------\n",
        "def calculate_iou(pred_mask, gt_mask):\n",
        "    pred_mask = (pred_mask > 0).cpu().numpy().astype(bool)\n",
        "    gt_mask = (gt_mask > 0).cpu().numpy().astype(bool)\n",
        "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
        "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
        "    return intersection / union if union != 0 else 1.0\n",
        "\n",
        "def calculate_dice(pred_mask, gt_mask):\n",
        "    pred_mask = (pred_mask > 0).cpu().numpy().astype(bool)\n",
        "    gt_mask = (gt_mask > 0).cpu().numpy().astype(bool)\n",
        "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
        "    sum_areas = pred_mask.sum() + gt_mask.sum()\n",
        "    return 2.0 * intersection / sum_areas if sum_areas != 0 else 1.0\n",
        "\n",
        "# -------------------------\n",
        "# Enhanced PolyGen2021 Dataset Class\n",
        "# -------------------------\n",
        "class PolyGen2021Dataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, split='train', transform=None, augment=True, img_size=256,\n",
        "                 split_ratio=(0.7, 0.15, 0.15), seed=42):\n",
        "        \"\"\"\n",
        "        Enhanced dataset class for PolyGen2021\n",
        "\n",
        "        Args:\n",
        "            images_dir: Path to images directory\n",
        "            masks_dir: Path to masks directory\n",
        "            split: 'train', 'val', or 'test'\n",
        "            transform: Image transformations\n",
        "            augment: Whether to use data augmentation\n",
        "            img_size: Target image size\n",
        "            split_ratio: Train/val/test split ratio as tuple (train, val, test)\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        self.augment = augment and split == 'train'\n",
        "        self.img_size = img_size\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "\n",
        "        if not os.path.exists(self.images_dir):\n",
        "            raise ValueError(f\"Images directory not found: {self.images_dir}\")\n",
        "        if not os.path.exists(self.masks_dir):\n",
        "            raise ValueError(f\"Masks directory not found: {self.masks_dir}\")\n",
        "\n",
        "        # Get all valid image files with verified mask pairs\n",
        "        self.image_mask_pairs = []\n",
        "\n",
        "        potential_images = sorted([f for f in os.listdir(self.images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "\n",
        "        print(f\"Finding valid image-mask pairs...\")\n",
        "        for img_name in tqdm(potential_images, desc=\"Checking image-mask pairs\"):\n",
        "            base_name = os.path.splitext(img_name)[0]\n",
        "            mask_found = False\n",
        "            mask_path = None\n",
        "\n",
        "            # Check for exact filename match with different extensions\n",
        "            for ext in ['.jpg', '.png', '.jpeg', '.tif']:\n",
        "                candidate = os.path.join(self.masks_dir, base_name + ext)\n",
        "                if os.path.exists(candidate):\n",
        "                    mask_found = True\n",
        "                    mask_path = candidate\n",
        "                    break\n",
        "\n",
        "            # Try to find mask by substring matching if exact match fails\n",
        "            if not mask_found:\n",
        "                for mask_file in os.listdir(self.masks_dir):\n",
        "                    # Check if base name is contained in mask filename or vice versa\n",
        "                    mask_base = os.path.splitext(mask_file)[0]\n",
        "                    if base_name in mask_base or mask_base in base_name:\n",
        "                        mask_path = os.path.join(self.masks_dir, mask_file)\n",
        "                        mask_found = True\n",
        "                        break\n",
        "\n",
        "            # If a matching mask is found, add to valid pairs\n",
        "            if mask_found:\n",
        "                self.image_mask_pairs.append((img_name, os.path.basename(mask_path)))\n",
        "\n",
        "        if not self.image_mask_pairs:\n",
        "            raise ValueError(f\"No valid image-mask pairs found. Check that images and masks directories are correct.\")\n",
        "\n",
        "        print(f\"Found {len(self.image_mask_pairs)} valid image-mask pairs\")\n",
        "\n",
        "        # Create train/val/test split based on the split_ratio\n",
        "        np.random.seed(seed)\n",
        "        indices = list(range(len(self.image_mask_pairs)))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_end = int(len(indices) * split_ratio[0])\n",
        "        val_end = int(len(indices) * (split_ratio[0] + split_ratio[1]))\n",
        "\n",
        "        if split == 'train':\n",
        "            self.indices = indices[:train_end]\n",
        "        elif split == 'val':\n",
        "            self.indices = indices[train_end:val_end]\n",
        "        else:  # test\n",
        "            self.indices = indices[val_end:]\n",
        "\n",
        "        print(f\"Created {split} dataset with {len(self.indices)} image-mask pairs\")\n",
        "\n",
        "        # Initialize augmentations\n",
        "        self.basic_transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, mask_name = self.image_mask_pairs[self.indices[idx]]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        # Apply augmentations for training with controlled randomness\n",
        "        if self.augment:\n",
        "            # Spatial transforms\n",
        "            if random.random() > 0.5:\n",
        "                image = TF.hflip(image)\n",
        "                mask = TF.hflip(mask)\n",
        "\n",
        "            if random.random() > 0.5:\n",
        "                image = TF.vflip(image)\n",
        "                mask = TF.vflip(mask)\n",
        "\n",
        "            # Rotation: More controlled rotation for medical images\n",
        "            if random.random() > 0.5:\n",
        "                angle = random.choice([0, 90, 180, 270])\n",
        "                image = TF.rotate(image, angle, fill=0)\n",
        "                mask = TF.rotate(mask, angle, fill=0)\n",
        "\n",
        "            # Intensity transformations\n",
        "            if random.random() > 0.5:\n",
        "                brightness = random.uniform(0.85, 1.15)\n",
        "                contrast = random.uniform(0.85, 1.15)\n",
        "                saturation = random.uniform(0.85, 1.15)\n",
        "\n",
        "                image = TF.adjust_brightness(image, brightness)\n",
        "                image = TF.adjust_contrast(image, contrast)\n",
        "                image = TF.adjust_saturation(image, saturation)\n",
        "\n",
        "            # Color jitter\n",
        "            if random.random() > 0.7:\n",
        "                color_jitter = transforms.ColorJitter(0.1, 0.1, 0.1, 0.05)\n",
        "                image = color_jitter(image)\n",
        "\n",
        "        # Apply basic transforms\n",
        "        image = self.basic_transform(image)\n",
        "\n",
        "        # Process mask\n",
        "        mask = TF.resize(mask, (self.img_size, self.img_size), interpolation=TF.InterpolationMode.NEAREST)\n",
        "        mask_array = np.array(mask)\n",
        "        mask_binary = (mask_array > 127).astype(np.int64)  # Threshold for binary mask\n",
        "        mask = torch.from_numpy(mask_binary).long()\n",
        "\n",
        "        # Apply normalization only after all other transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# -------------------------\n",
        "# Dataset Utility Functions\n",
        "# -------------------------\n",
        "def explore_dataset_structure(base_dir):\n",
        "    \"\"\"Explore the dataset structure to determine the actual paths\"\"\"\n",
        "    print(f\"Exploring dataset structure at {base_dir}...\")\n",
        "\n",
        "    if not os.path.exists(base_dir):\n",
        "        print(f\"Base directory {base_dir} does not exist\")\n",
        "        return None, None\n",
        "\n",
        "    # Print the directory structure to understand what we're working with\n",
        "    print(\"Directory structure:\")\n",
        "\n",
        "    found_images_dir = None\n",
        "    found_masks_dir = None\n",
        "\n",
        "    # Level 1 directories\n",
        "    for root_item in os.listdir(base_dir):\n",
        "        root_path = os.path.join(base_dir, root_item)\n",
        "        if os.path.isdir(root_path):\n",
        "            print(f\"- {root_item}/\")\n",
        "\n",
        "            # Level 2 directories\n",
        "            for sub_item in os.listdir(root_path):\n",
        "                sub_path = os.path.join(root_path, sub_item)\n",
        "                if os.path.isdir(sub_path):\n",
        "                    print(f\"  - {sub_item}/\")\n",
        "\n",
        "                    # Check for 'images' and 'masks' directories\n",
        "                    if sub_item == 'images' and not found_images_dir:\n",
        "                        found_images_dir = sub_path\n",
        "                        print(f\"    Found images directory: {found_images_dir}\")\n",
        "\n",
        "                    elif sub_item == 'masks' and not found_masks_dir:\n",
        "                        found_masks_dir = sub_path\n",
        "                        print(f\"    Found masks directory: {found_masks_dir}\")\n",
        "\n",
        "                    # Level 3 directories (for nested structures)\n",
        "                    else:\n",
        "                        for third_item in os.listdir(sub_path):\n",
        "                            third_path = os.path.join(sub_path, third_item)\n",
        "                            if os.path.isdir(third_path):\n",
        "                                print(f\"    - {third_item}/\")\n",
        "\n",
        "                                # Check for deeply nested image/mask directories\n",
        "                                if third_item == 'images' and not found_images_dir:\n",
        "                                    found_images_dir = third_path\n",
        "                                    print(f\"      Found images directory: {found_images_dir}\")\n",
        "\n",
        "                                elif third_item == 'masks' and not found_masks_dir:\n",
        "                                    found_masks_dir = third_path\n",
        "                                    print(f\"      Found masks directory: {found_masks_dir}\")\n",
        "\n",
        "    # Check if we found special folder structure from error message\n",
        "    if not found_images_dir and os.path.exists(os.path.join(base_dir, \"PolypGen2021_MultiCenterData_v3\", \"positive\", \"images\")):\n",
        "        found_images_dir = os.path.join(base_dir, \"PolypGen2021_MultiCenterData_v3\", \"positive\", \"images\")\n",
        "        print(f\"Found images directory: {found_images_dir}\")\n",
        "\n",
        "    if not found_masks_dir and os.path.exists(os.path.join(base_dir, \"PolypGen2021_MultiCenterData_v3\", \"positive\", \"masks\")):\n",
        "        found_masks_dir = os.path.join(base_dir, \"PolypGen2021_MultiCenterData_v3\", \"positive\", \"masks\")\n",
        "        print(f\"Found masks directory: {found_masks_dir}\")\n",
        "\n",
        "    if found_images_dir:\n",
        "        # Check image count\n",
        "        image_files = [f for f in os.listdir(found_images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "        print(f\"Found {len(image_files)} images in {found_images_dir}\")\n",
        "        if image_files:\n",
        "            print(f\"Sample image names: {image_files[:3]}\")\n",
        "\n",
        "    if found_masks_dir:\n",
        "        # Check mask count\n",
        "        mask_files = [f for f in os.listdir(found_masks_dir) if f.endswith(('.jpg', '.png', '.jpeg', '.tif'))]\n",
        "        print(f\"Found {len(mask_files)} masks in {found_masks_dir}\")\n",
        "        if mask_files:\n",
        "            print(f\"Sample mask names: {mask_files[:3]}\")\n",
        "\n",
        "    return found_images_dir, found_masks_dir\n",
        "\n",
        "def find_dataset_paths():\n",
        "    \"\"\"Find the correct matching image and mask paths for PolyGen2021\"\"\"\n",
        "    kaggle_input_path = \"/kaggle/input/polypgen2021\"\n",
        "\n",
        "    if os.path.exists(kaggle_input_path):\n",
        "        print(f\"Found PolyGen2021 dataset at {kaggle_input_path}\")\n",
        "\n",
        "        # Based on the error message, we know the structure of the dataset\n",
        "        if os.path.exists(\"/kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3\"):\n",
        "            # For polyp segmentation, we need the positive directory (images with polyps and their masks)\n",
        "            images_dir = \"/kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images\"\n",
        "            masks_dir = \"/kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks\"\n",
        "\n",
        "            if os.path.exists(images_dir) and os.path.exists(masks_dir):\n",
        "                print(f\"Using positive images from: {images_dir}\")\n",
        "                print(f\"Using masks from: {masks_dir}\")\n",
        "\n",
        "                # Verify some image-mask pairs exist\n",
        "                image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "                mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "\n",
        "                print(f\"Found {len(image_files)} positive images\")\n",
        "                print(f\"Found {len(mask_files)} masks\")\n",
        "\n",
        "                if image_files and mask_files:\n",
        "                    # Show some sample names to help with debugging\n",
        "                    print(f\"Sample image names: {image_files[:3]}\")\n",
        "                    print(f\"Sample mask names: {mask_files[:3]}\")\n",
        "\n",
        "                return images_dir, masks_dir\n",
        "            else:\n",
        "                if not os.path.exists(images_dir):\n",
        "                    print(f\"WARNING: Positive images directory not found at {images_dir}\")\n",
        "                if not os.path.exists(masks_dir):\n",
        "                    print(f\"WARNING: Masks directory not found at {masks_dir}\")\n",
        "\n",
        "        # If the specific structure wasn't found, explore the dataset\n",
        "        print(\"Exploring dataset structure to find matching image and mask directories...\")\n",
        "        for root, dirs, files in os.walk(kaggle_input_path):\n",
        "            if os.path.basename(root) == \"images\" and \"positive\" in root:\n",
        "                potential_images_dir = root\n",
        "                # Look for a parallel masks directory\n",
        "                potential_masks_dir = os.path.join(os.path.dirname(root), \"masks\")\n",
        "                if os.path.exists(potential_masks_dir):\n",
        "                    print(f\"Found matching directories:\")\n",
        "                    print(f\"Images: {potential_images_dir}\")\n",
        "                    print(f\"Masks: {potential_masks_dir}\")\n",
        "                    return potential_images_dir, potential_masks_dir\n",
        "\n",
        "    # If automated finding fails, ask for manual input\n",
        "    print(\"Could not find matching image and mask directories automatically.\")\n",
        "    print(\"Please enter the paths manually:\")\n",
        "\n",
        "    print(\"\\nAvailable dataset contents:\")\n",
        "    for root, dirs, files in os.walk(\"/kaggle/input/polypgen2021\", topdown=True, maxdepth=3):\n",
        "        print(f\"- {root}\")\n",
        "        if len(files) > 0:\n",
        "            print(f\"  ({len(files)} files, first few: {files[:3]})\")\n",
        "\n",
        "    images_dir = input(\"Enter the path to the POSITIVE images directory: \")\n",
        "    masks_dir = input(\"Enter the path to the masks directory: \")\n",
        "\n",
        "    if os.path.exists(images_dir) and os.path.exists(masks_dir):\n",
        "        return images_dir, masks_dir\n",
        "    else:\n",
        "        raise ValueError(\"Invalid directory paths provided\")\n",
        "\n",
        "# -------------------------\n",
        "# Enhanced Data Loaders\n",
        "# -------------------------\n",
        "def get_data_loaders(images_dir, masks_dir, batch_size=8, img_size=256,\n",
        "                     split_ratio=(0.7, 0.15, 0.15), num_workers=4, seed=42):\n",
        "    \"\"\"\n",
        "    Create data loaders for train, validation, and test sets\n",
        "\n",
        "    Args:\n",
        "        images_dir: Directory containing images\n",
        "        masks_dir: Directory containing masks\n",
        "        batch_size: Batch size for training\n",
        "        img_size: Size of input images\n",
        "        split_ratio: Tuple of (train, val, test) split ratios\n",
        "        num_workers: Number of worker processes for data loading\n",
        "        seed: Random seed for reproducibility\n",
        "    \"\"\"\n",
        "    # Define normalization pipeline\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = PolyGen2021Dataset(\n",
        "        images_dir=images_dir,\n",
        "        masks_dir=masks_dir,\n",
        "        split='train',\n",
        "        transform=normalize,\n",
        "        augment=True,\n",
        "        img_size=img_size,\n",
        "        split_ratio=split_ratio,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    val_dataset = PolyGen2021Dataset(\n",
        "        images_dir=images_dir,\n",
        "        masks_dir=masks_dir,\n",
        "        split='val',\n",
        "        transform=normalize,\n",
        "        augment=False,\n",
        "        img_size=img_size,\n",
        "        split_ratio=split_ratio,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    test_dataset = PolyGen2021Dataset(\n",
        "        images_dir=images_dir,\n",
        "        masks_dir=masks_dir,\n",
        "        split='test',\n",
        "        transform=normalize,\n",
        "        augment=False,\n",
        "        img_size=img_size,\n",
        "        split_ratio=split_ratio,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True  # Prevent issues with batch norm on small last batch\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, (len(train_dataset), len(val_dataset), len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-12T12:34:09.061944Z",
          "iopub.status.busy": "2025-06-12T12:34:09.061656Z",
          "iopub.status.idle": "2025-06-12T12:34:09.266410Z",
          "shell.execute_reply": "2025-06-12T12:34:09.265675Z",
          "shell.execute_reply.started": "2025-06-12T12:34:09.061923Z"
        },
        "trusted": true,
        "id": "HFHblg6D_6oa"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Enhanced Training Function\n",
        "# -------------------------\n",
        "def train_model(model, train_loader, val_loader, num_epochs=200,\n",
        "                learning_rate=1.8e-4, device='cuda', checkpoint_dir='/kaggle/working/checkpoints',\n",
        "                patience=15, scheduler_type='cosine'):\n",
        "    \"\"\"\n",
        "    Enhanced training function with various optimizations\n",
        "\n",
        "    Args:\n",
        "        model: Model to train\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        num_epochs: Number of epochs to train for\n",
        "        learning_rate: Initial learning rate\n",
        "        device: Device to train on\n",
        "        checkpoint_dir: Directory to save checkpoints\n",
        "        patience: Early stopping patience\n",
        "        scheduler_type: Type of learning rate scheduler to use\n",
        "    \"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Combined loss with dice and focal loss components\n",
        "    criterion = CombinedLoss(bce_weight=0.3, dice_weight=0.7, focal_weight=0.3)\n",
        "    deep_criterion = DeepSupervisionLoss(main_weight=0.8, deep2_weight=0.1, deep3_weight=0.1)\n",
        "\n",
        "    # Optimizer with weight decay and grad clipping\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=1e-5,\n",
        "        betas=(0.9, 0.95)\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    if scheduler_type == 'cosine':\n",
        "        def lr_lambda(epoch):\n",
        "            if epoch < 10:  # Warm-up period\n",
        "                return 0.1 + 0.9 * (epoch / 10.0)\n",
        "            else:  # Cosine decay\n",
        "                return 0.5 * (1.0 + math.cos(math.pi * (epoch - 10) / (num_epochs - 10)))\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "    elif scheduler_type == 'plateau':\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6\n",
        "        )\n",
        "    else:\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = GradScaler('cuda')\n",
        "\n",
        "    # Training tracking\n",
        "    best_iou = 0.0\n",
        "    best_dice = 0.0\n",
        "    best_epoch = 0\n",
        "    early_stop_counter = 0\n",
        "    history = {\n",
        "        'train_losses': [],\n",
        "        'val_losses': [],\n",
        "        'val_ious': [],\n",
        "        'val_dices': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    # Training loop with progress bar\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_iou = 0.0\n",
        "        train_dice = 0.0\n",
        "        train_batches = 0\n",
        "\n",
        "        # Progress bar for training\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(progress_bar):\n",
        "            images, masks = images.to(device, non_blocking=True), masks.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
        "\n",
        "            with autocast('cuda'):\n",
        "                if epoch < 15:\n",
        "                    outputs = model(images, return_deep=False)\n",
        "                    loss = criterion(outputs, masks)\n",
        "                else:\n",
        "                    outputs = model(images, return_deep=True)\n",
        "                    if epoch < 25:\n",
        "                        deep_criterion.main_weight = 0.9\n",
        "                        deep_criterion.deep2_weight = 0.05\n",
        "                        deep_criterion.deep3_weight = 0.05\n",
        "                    else:\n",
        "                        deep_criterion.main_weight = 0.8\n",
        "                        deep_criterion.deep2_weight = 0.1\n",
        "                        deep_criterion.deep3_weight = 0.1\n",
        "                    loss = deep_criterion(outputs, masks)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Update metrics\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate training metrics occasionally for progress monitoring\n",
        "            if batch_idx % 20 == 0:\n",
        "                with torch.no_grad():\n",
        "                    main_output = outputs[0] if isinstance(outputs, tuple) else outputs\n",
        "                    predictions = torch.argmax(main_output, dim=1)\n",
        "                    batch_iou = 0.0\n",
        "                    batch_dice = 0.0\n",
        "\n",
        "                    for i in range(min(4, images.size(0))):  # Check just a few samples to save time\n",
        "                        pred_mask = predictions[i]\n",
        "                        gt_mask = masks[i]\n",
        "                        batch_iou += calculate_iou(pred_mask, gt_mask)\n",
        "                        batch_dice += calculate_dice(pred_mask, gt_mask)\n",
        "\n",
        "                    batch_iou /= min(4, images.size(0))\n",
        "                    batch_dice /= min(4, images.size(0))\n",
        "                    train_iou += batch_iou\n",
        "                    train_dice += batch_dice\n",
        "                    train_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
        "            })\n",
        "\n",
        "            if batch_idx % 5 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_iou = 0.0\n",
        "        val_dice = 0.0\n",
        "        num_val_samples = 0\n",
        "\n",
        "        # Progress bar for validation\n",
        "        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_progress:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images, return_deep=(epoch >= 15))\n",
        "\n",
        "                if epoch < 15:\n",
        "                    loss = criterion(outputs, masks)\n",
        "                else:\n",
        "                    loss = deep_criterion(outputs, masks)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                main_output = outputs[0] if isinstance(outputs, tuple) else outputs\n",
        "                predictions = torch.argmax(main_output, dim=1)\n",
        "\n",
        "                for i in range(images.size(0)):\n",
        "                    pred_mask = predictions[i]\n",
        "                    gt_mask = masks[i]\n",
        "                    iou = calculate_iou(pred_mask, gt_mask)\n",
        "                    dice = calculate_dice(pred_mask, gt_mask)\n",
        "                    val_iou += iou\n",
        "                    val_dice += dice\n",
        "                    num_val_samples += 1\n",
        "\n",
        "                val_progress.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_iou = val_iou / num_val_samples\n",
        "        avg_val_dice = val_dice / num_val_samples\n",
        "\n",
        "        if train_batches > 0:\n",
        "            avg_train_iou = train_iou / train_batches\n",
        "            avg_train_dice = train_dice / train_batches\n",
        "        else:\n",
        "            avg_train_iou = 0\n",
        "            avg_train_dice = 0\n",
        "\n",
        "        # Update history\n",
        "        history['train_losses'].append(avg_train_loss)\n",
        "        history['val_losses'].append(avg_val_loss)\n",
        "        history['val_ious'].append(avg_val_iou)\n",
        "        history['val_dices'].append(avg_val_dice)\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        # Update scheduler\n",
        "        if scheduler_type == 'plateau':\n",
        "            scheduler.step(avg_val_iou)  # Step with validation IoU for plateau scheduler\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f}, IoU: {avg_train_iou:.4f}, Dice: {avg_train_dice:.4f}')\n",
        "        print(f'  Val Loss: {avg_val_loss:.4f}, IoU: {avg_val_iou:.4f}, Dice: {avg_val_dice:.4f}')\n",
        "        print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        # Check for best model (using IoU)\n",
        "        if avg_val_iou > best_iou:\n",
        "            best_iou = avg_val_iou\n",
        "            best_dice = avg_val_dice\n",
        "            best_epoch = epoch\n",
        "            early_stop_counter = 0\n",
        "\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_iou': best_iou,\n",
        "                'best_dice': best_dice,\n",
        "                'history': history\n",
        "            }, os.path.join(checkpoint_dir, 'best_model_polygen.pth'))\n",
        "            print(f'  New best model saved with IoU: {best_iou:.4f}, Dice: {best_dice:.4f}')\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            print(f'  No improvement for {early_stop_counter} epochs. Best IoU: {best_iou:.4f} at epoch {best_epoch+1}')\n",
        "\n",
        "            # Save regular checkpoint every 10 epochs\n",
        "            if epoch % 10 == 0:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'best_iou': best_iou,\n",
        "                    'history': history\n",
        "                }, os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
        "\n",
        "        # Plot progress every 10 epochs\n",
        "        if epoch % 10 == 9 or epoch == num_epochs - 1:\n",
        "            plot_training_progress(history, save_path=os.path.join(checkpoint_dir, f'progress_epoch_{epoch+1}.png'))\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stop_counter >= patience:\n",
        "            print(f\"Early stopping after {patience} epochs without improvement\")\n",
        "            break\n",
        "\n",
        "        print('-' * 50)\n",
        "\n",
        "    print(f\"Training complete! Best validation IoU: {best_iou:.4f}, Dice: {best_dice:.4f} at epoch {best_epoch+1}\")\n",
        "    return history, best_iou, best_dice, best_epoch\n",
        "\n",
        "# -------------------------\n",
        "# Enhanced Visualization Functions\n",
        "# -------------------------\n",
        "def visualize_predictions(model, test_loader, device='cuda', num_samples=5, save_path=None):\n",
        "    \"\"\"Enhanced prediction visualization with overlay and error analysis\"\"\"\n",
        "    model.eval()\n",
        "    all_images = []\n",
        "    all_masks = []\n",
        "    all_preds = []\n",
        "\n",
        "    # Get samples\n",
        "    with torch.no_grad():\n",
        "        for images, masks in test_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images, return_deep=False)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Add to collections\n",
        "            all_images.extend(images.cpu())\n",
        "            all_masks.extend(masks.cpu())\n",
        "            all_preds.extend(predictions.cpu())\n",
        "\n",
        "            if len(all_images) >= num_samples:\n",
        "                break\n",
        "\n",
        "    # Only take the requested number of samples\n",
        "    all_images = all_images[:num_samples]\n",
        "    all_masks = all_masks[:num_samples]\n",
        "    all_preds = all_preds[:num_samples]\n",
        "\n",
        "    # Denormalize images\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "    std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "    # Create visualization grid\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "\n",
        "    # If only one sample, wrap axes in a list to make indexing consistent\n",
        "    if num_samples == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Get image, mask, and prediction\n",
        "        img = all_images[i] * std.view(3, 1, 1) + mean.view(3, 1, 1)\n",
        "        img = torch.clamp(img, 0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "        mask = all_masks[i].numpy()\n",
        "        pred = all_preds[i].numpy()\n",
        "\n",
        "        # Calculate metrics\n",
        "        iou_score = calculate_iou(all_preds[i], all_masks[i])\n",
        "        dice_score = calculate_dice(all_preds[i], all_masks[i])\n",
        "\n",
        "        # Create overlay for better visualization\n",
        "        # Green: True Positive, Blue: False Negative, Red: False Positive\n",
        "        overlay = np.zeros((*img.shape[:2], 4))\n",
        "\n",
        "        # True positive (both pred and gt are 1) - green\n",
        "        true_positive = np.logical_and(pred == 1, mask == 1)\n",
        "        overlay[true_positive] = [0, 1, 0, 0.5]  # Semi-transparent green\n",
        "\n",
        "        # False positive (pred is 1 but gt is 0) - red\n",
        "        false_positive = np.logical_and(pred == 1, mask == 0)\n",
        "        overlay[false_positive] = [1, 0, 0, 0.5]  # Semi-transparent red\n",
        "\n",
        "        # False negative (pred is 0 but gt is 1) - blue\n",
        "        false_negative = np.logical_and(pred == 0, mask == 1)\n",
        "        overlay[false_negative] = [0, 0, 1, 0.5]  # Semi-transparent blue\n",
        "\n",
        "        # Plot original image\n",
        "        axes[i, 0].imshow(img)\n",
        "        axes[i, 0].set_title('Original Image')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Plot ground truth\n",
        "        axes[i, 1].imshow(mask, cmap='gray')\n",
        "        axes[i, 1].set_title('Ground Truth')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Plot prediction\n",
        "        axes[i, 2].imshow(pred, cmap='gray')\n",
        "        axes[i, 2].set_title('Prediction')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "        # Plot overlay on original image\n",
        "        axes[i, 3].imshow(img)\n",
        "        axes[i, 3].imshow(overlay)\n",
        "        axes[i, 3].set_title(f'Overlay (IoU: {iou_score:.4f}, Dice: {dice_score:.4f})')\n",
        "        axes[i, 3].axis('off')\n",
        "\n",
        "        # Add legend to the overlay\n",
        "        legend_elements = [\n",
        "            plt.Rectangle((0, 0), 1, 1, color=(0, 1, 0, 0.5), label='True Positive'),\n",
        "            plt.Rectangle((0, 0), 1, 1, color=(1, 0, 0, 0.5), label='False Positive'),\n",
        "            plt.Rectangle((0, 0), 1, 1, color=(0, 0, 1, 0.5), label='False Negative')\n",
        "        ]\n",
        "\n",
        "        # Place legend outside the last subplot\n",
        "        if i == 0:  # Only add legend to the first row\n",
        "            axes[i, 3].legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return all_images, all_masks, all_preds\n",
        "\n",
        "def visualize_contours(model, test_loader, device='cuda', num_samples=5, save_path=None):\n",
        "    \"\"\"Visualize contour extraction and feature maps\"\"\"\n",
        "    model.eval()\n",
        "    images, masks = next(iter(test_loader))\n",
        "    images = images.to(device)[:num_samples]\n",
        "\n",
        "    # Extract intermediate activations\n",
        "    with torch.no_grad():\n",
        "        # Get encoder features\n",
        "        enc1 = model.encoder1(images)\n",
        "        enc1_pool = model.pool1(enc1)\n",
        "        enc2 = model.encoder2(enc1_pool)\n",
        "        enc2_pool = model.pool2(enc2)\n",
        "        enc3 = model.encoder3(enc2_pool)\n",
        "        enc3_pool = model.pool3(enc3)\n",
        "\n",
        "        # Get contour features\n",
        "        bridge_out = model.bridge_down(enc3_pool)\n",
        "        b, c, h, w = bridge_out.size()\n",
        "        mamba_input = bridge_out.permute(0, 2, 3, 1).reshape(b, h * w, c)\n",
        "\n",
        "        # Just visualize the first mamba block's contour extractor\n",
        "        contour_spatial = model.mamba_blocks[0].contour_extractor(\n",
        "            mamba_input.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
        "        )\n",
        "\n",
        "    # Prepare visualization\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "\n",
        "    # If only one sample, wrap axes in a list to make indexing consistent\n",
        "    if num_samples == 1:\n",
        "        axes = np.array([axes])\n",
        "\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Original image\n",
        "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "        img = img * std + mean\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        # Encoder features (high level)\n",
        "        enc_features = enc3[i].mean(dim=0).cpu().numpy()\n",
        "        enc_features = (enc_features - enc_features.min()) / (enc_features.max() - enc_features.min() + 1e-8)\n",
        "\n",
        "        # Contour features\n",
        "        contour = contour_spatial[i].mean(dim=0).cpu().numpy()\n",
        "        contour = (contour - contour.min()) / (contour.max() - contour.min() + 1e-8)\n",
        "\n",
        "        # Heatmap overlay\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * contour), cv2.COLORMAP_JET)\n",
        "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) / 255.0\n",
        "\n",
        "        # Resize heatmap to match original image size\n",
        "        heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "\n",
        "        # Create blend of original image and heatmap\n",
        "        blend = 0.7 * img + 0.3 * heatmap_resized\n",
        "\n",
        "        # Plot each visualization\n",
        "        axes[i, 0].imshow(img)\n",
        "        axes[i, 0].set_title('Input Image')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 1].imshow(enc_features, cmap='viridis')\n",
        "        axes[i, 1].set_title('Encoder Features')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        axes[i, 2].imshow(contour, cmap='magma')\n",
        "        axes[i, 2].set_title('Contour Features')\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "        axes[i, 3].imshow(blend)\n",
        "        axes[i, 3].set_title('Contour Heatmap Overlay')\n",
        "        axes[i, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_progress(history, save_path=None, figsize=(18, 6)):\n",
        "    \"\"\"Plot detailed training progress with multiple metrics\"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(history['train_losses'], label='Train Loss')\n",
        "    plt.plot(history['val_losses'], label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot validation IoU\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(history['val_ious'], label='IoU')\n",
        "    if 'val_dices' in history:\n",
        "        plt.plot(history['val_dices'], label='Dice')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.title('Validation Metrics')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot learning rate\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(history['lr'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# Enhanced Evaluation Metrics\n",
        "# -------------------------\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, bce_weight=0.3, dice_weight=0.5, focal_weight=0.2, gamma=2.0):\n",
        "        \"\"\"\n",
        "        Combined loss function with BCE, Dice, and Focal loss components\n",
        "\n",
        "        Args:\n",
        "            bce_weight: Weight for binary cross-entropy loss\n",
        "            dice_weight: Weight for dice loss\n",
        "            focal_weight: Weight for focal loss\n",
        "            gamma: Focal loss focusing parameter\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "        self.focal_weight = focal_weight\n",
        "        self.gamma = gamma\n",
        "        self.bce_loss = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # BCE Loss\n",
        "        bce = self.bce_loss(inputs, targets)\n",
        "\n",
        "        # Dice Loss\n",
        "        inputs_soft = F.softmax(inputs, dim=1)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n",
        "\n",
        "        intersection = (inputs_soft * targets_one_hot).sum(dim=(2, 3))\n",
        "        cardinality = inputs_soft.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))\n",
        "        dice = (2. * intersection / (cardinality + 1e-6)).mean()\n",
        "        dice_loss = 1 - dice\n",
        "\n",
        "        # Focal Loss (for hard examples)\n",
        "        if self.focal_weight > 0:\n",
        "            # Calculate focal factor\n",
        "            pt = torch.exp(-bce)\n",
        "            focal_factor = (1 - pt) ** self.gamma\n",
        "            focal_loss = focal_factor * bce\n",
        "\n",
        "            # Return combined loss\n",
        "            return self.bce_weight * bce + self.dice_weight * dice_loss + self.focal_weight * focal_loss.mean()\n",
        "        else:\n",
        "            return self.bce_weight * bce + self.dice_weight * dice_loss\n",
        "\n",
        "# Enhanced evaluation metrics and test function\n",
        "def calculate_metrics(pred_mask, gt_mask):\n",
        "    \"\"\"Calculate multiple evaluation metrics\"\"\"\n",
        "    pred_mask = pred_mask.cpu().numpy().astype(bool)\n",
        "    gt_mask = gt_mask.cpu().numpy().astype(bool)\n",
        "\n",
        "    # Intersection and union\n",
        "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
        "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
        "\n",
        "    # Pixel counts\n",
        "    gt_pixels = gt_mask.sum()\n",
        "    pred_pixels = pred_mask.sum()\n",
        "\n",
        "    # Calculate metrics\n",
        "    iou = intersection / union if union > 0 else 1.0\n",
        "    dice = 2 * intersection / (gt_pixels + pred_pixels) if (gt_pixels + pred_pixels) > 0 else 1.0\n",
        "\n",
        "    # Precision and recall\n",
        "    precision = intersection / pred_pixels if pred_pixels > 0 else 1.0\n",
        "    recall = intersection / gt_pixels if gt_pixels > 0 else 1.0\n",
        "\n",
        "    # F1 score (equivalent to Dice but calculated differently)\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'iou': float(iou),\n",
        "        'dice': float(dice),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1)\n",
        "    }\n",
        "\n",
        "def test_model_comprehensive(model, test_loader, device='cuda', use_tta=True, use_crf=False):\n",
        "    \"\"\"Comprehensive model evaluation with TTA and detailed metrics\"\"\"\n",
        "    model.eval()\n",
        "    all_metrics = []\n",
        "    all_metrics_by_size = {'small': [], 'medium': [], 'large': []}\n",
        "\n",
        "    # For ROC curve\n",
        "    all_probs = []\n",
        "    all_gt = []\n",
        "\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in progress_bar:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Standard prediction\n",
        "            outputs = model(images, return_deep=False)\n",
        "\n",
        "            # Test-time augmentation\n",
        "            if use_tta:\n",
        "                # Horizontal flip\n",
        "                images_hflip = torch.flip(images, [3])\n",
        "                outputs_hflip = model(images_hflip, return_deep=False)\n",
        "                outputs_hflip = torch.flip(outputs_hflip, [3])\n",
        "\n",
        "                # Vertical flip\n",
        "                images_vflip = torch.flip(images, [2])\n",
        "                outputs_vflip = model(images_vflip, return_deep=False)\n",
        "                outputs_vflip = torch.flip(outputs_vflip, [2])\n",
        "\n",
        "                # Combined prediction\n",
        "                outputs = (outputs + outputs_hflip + outputs_vflip) / 3\n",
        "\n",
        "            # Store probability maps for ROC analysis\n",
        "            probs = F.softmax(outputs, dim=1)[:, 1].cpu().numpy().flatten()\n",
        "            all_probs.extend(probs)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            for i in range(images.size(0)):\n",
        "                pred_mask = predictions[i]\n",
        "                gt_mask = masks[i]\n",
        "\n",
        "                # Store ground truth for ROC analysis\n",
        "                all_gt.extend(gt_mask.cpu().numpy().flatten())\n",
        "\n",
        "                # Apply CRF post-processing if requested\n",
        "                if use_crf:\n",
        "                    try:\n",
        "                        import pydensecrf.densecrf as dcrf\n",
        "                        from pydensecrf.utils import unary_from_softmax\n",
        "\n",
        "                        # Create CRF input\n",
        "                        image = images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "                        image = (image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
        "                        image = (image * 255).astype(np.uint8)\n",
        "\n",
        "                        # Get probability map\n",
        "                        prob = F.softmax(outputs[i], dim=0).cpu().numpy()\n",
        "\n",
        "                        # Create CRF model\n",
        "                        d = dcrf.DenseCRF2D(image.shape[1], image.shape[0], 2)\n",
        "                        U = unary_from_softmax(prob)\n",
        "                        d.setUnaryEnergy(U)\n",
        "\n",
        "                        # Add pairwise terms\n",
        "                        d.addPairwiseGaussian(sxy=3, compat=3)\n",
        "                        d.addPairwiseBilateral(sxy=80, srgb=13, rgbim=image, compat=10)\n",
        "\n",
        "                        # Perform inference\n",
        "                        Q = d.inference(5)\n",
        "                        pred_mask = torch.from_numpy(np.argmax(Q, axis=0).reshape(gt_mask.shape)).to(device)\n",
        "                    except ImportError:\n",
        "                        print(\"Warning: pydensecrf not available. Skipping CRF post-processing.\")\n",
        "\n",
        "                # Calculate comprehensive metrics\n",
        "                metrics = calculate_metrics(pred_mask, gt_mask)\n",
        "                all_metrics.append(metrics)\n",
        "\n",
        "                # Categorize by polyp size\n",
        "                gt_size = gt_mask.sum().item() / (gt_mask.shape[0] * gt_mask.shape[1])\n",
        "\n",
        "                if gt_size < 0.05:  # Small polyps (less than 5% of image)\n",
        "                    all_metrics_by_size['small'].append(metrics)\n",
        "                elif gt_size < 0.15:  # Medium polyps (5-15% of image)\n",
        "                    all_metrics_by_size['medium'].append(metrics)\n",
        "                else:  # Large polyps (>15% of image)\n",
        "                    all_metrics_by_size['large'].append(metrics)\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_metrics = {}\n",
        "    for key in all_metrics[0].keys():\n",
        "        avg_metrics[key] = np.mean([m[key] for m in all_metrics])\n",
        "        avg_metrics[f\"{key}_std\"] = np.std([m[key] for m in all_metrics])\n",
        "\n",
        "    # Calculate average metrics by size\n",
        "    avg_metrics_by_size = {}\n",
        "    for size, metrics_list in all_metrics_by_size.items():\n",
        "        if metrics_list:  # Only calculate if there are samples in this category\n",
        "            avg_metrics_by_size[size] = {}\n",
        "            for key in metrics_list[0].keys():\n",
        "                avg_metrics_by_size[size][key] = np.mean([m[key] for m in metrics_list])\n",
        "                avg_metrics_by_size[size][f\"{key}_std\"] = np.std([m[key] for m in metrics_list])\n",
        "            avg_metrics_by_size[size]['count'] = len(metrics_list)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(f\"Number of samples: {len(all_metrics)}\")\n",
        "    print(f\"IoU: {avg_metrics['iou']:.4f} ± {avg_metrics['iou_std']:.4f}\")\n",
        "    print(f\"Dice: {avg_metrics['dice']:.4f} ± {avg_metrics['dice_std']:.4f}\")\n",
        "    print(f\"Precision: {avg_metrics['precision']:.4f} ± {avg_metrics['precision_std']:.4f}\")\n",
        "    print(f\"Recall: {avg_metrics['recall']:.4f} ± {avg_metrics['recall_std']:.4f}\")\n",
        "    print(f\"F1 Score: {avg_metrics['f1']:.4f} ± {avg_metrics['f1_std']:.4f}\")\n",
        "\n",
        "    print(\"\\nResults by polyp size:\")\n",
        "    for size, metrics in avg_metrics_by_size.items():\n",
        "        if metrics:\n",
        "            print(f\"\\n{size.capitalize()} polyps ({metrics['count']} samples):\")\n",
        "            print(f\"  IoU: {metrics['iou']:.4f} ± {metrics['iou_std']:.4f}\")\n",
        "            print(f\"  Dice: {metrics['dice']:.4f} ± {metrics['dice_std']:.4f}\")\n",
        "\n",
        "    # Calculate high IoU percentage\n",
        "    high_iou_count = sum(1 for m in all_metrics if m['iou'] > 0.9)\n",
        "    high_iou_percent = high_iou_count / len(all_metrics) * 100 if all_metrics else 0\n",
        "    print(f\"\\nSamples with IoU > 0.9: {high_iou_count} ({high_iou_percent:.1f}%)\")\n",
        "\n",
        "    # Return all collected data for further analysis\n",
        "    return {\n",
        "        'avg_metrics': avg_metrics,\n",
        "        'metrics_by_size': avg_metrics_by_size,\n",
        "        'all_metrics': all_metrics,\n",
        "        'all_probs': np.array(all_probs),\n",
        "        'all_gt': np.array(all_gt)\n",
        "    }\n",
        "\n",
        "def plot_roc_curve(all_probs, all_gt, save_path=None):\n",
        "    \"\"\"Plot ROC curve from classification results\"\"\"\n",
        "    from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "\n",
        "    # Calculate ROC curve and area\n",
        "    fpr, tpr, _ = roc_curve(all_gt, all_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Calculate PR curve and area\n",
        "    precision, recall, _ = precision_recall_curve(all_gt, all_probs)\n",
        "    pr_auc = average_precision_score(all_gt, all_probs)\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Plot ROC curve\n",
        "    ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    ax1.set_xlim([0.0, 1.0])\n",
        "    ax1.set_ylim([0.0, 1.05])\n",
        "    ax1.set_xlabel('False Positive Rate')\n",
        "    ax1.set_ylabel('True Positive Rate')\n",
        "    ax1.set_title('Receiver Operating Characteristic (ROC)')\n",
        "    ax1.legend(loc=\"lower right\")\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot PR curve\n",
        "    ax2.plot(recall, precision, color='green', lw=2, label=f'PR curve (area = {pr_auc:.3f})')\n",
        "    ax2.set_xlim([0.0, 1.0])\n",
        "    ax2.set_ylim([0.0, 1.05])\n",
        "    ax2.set_xlabel('Recall')\n",
        "    ax2.set_ylabel('Precision')\n",
        "    ax2.set_title('Precision-Recall Curve')\n",
        "    ax2.legend(loc=\"lower left\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return roc_auc, pr_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-12T12:34:09.267941Z",
          "iopub.status.busy": "2025-06-12T12:34:09.267657Z"
        },
        "trusted": true,
        "id": "-vKfDjGc_6oc",
        "outputId": "71cef0da-82d9-47d3-ca1e-4b1eb01bdabe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla P100-PCIE-16GB\n",
            "Memory: 17.06 GB\n",
            "Finding PolyGen2021 dataset in Kaggle environment...\n",
            "Found PolyGen2021 dataset at /kaggle/input/polypgen2021\n",
            "Using positive images from: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images\n",
            "Using masks from: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks\n",
            "Found 3762 positive images\n",
            "Found 3762 masks\n",
            "Sample image names: ['C1_100H0050.jpg', 'C1_100S0001.jpg', 'C1_100S0003.jpg']\n",
            "Sample mask names: ['C1_100H0050.jpg', 'C1_100S0001.jpg', 'C1_100S0003.jpg']\n",
            "Using images from: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images\n",
            "Using masks from: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks\n",
            "Creating data loaders...\n",
            "Finding valid image-mask pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checking image-mask pairs: 100%|██████████| 3762/3762 [00:07<00:00, 478.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3762 valid image-mask pairs\n",
            "Created train dataset with 2633 image-mask pairs\n",
            "Finding valid image-mask pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checking image-mask pairs: 100%|██████████| 3762/3762 [00:01<00:00, 2477.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3762 valid image-mask pairs\n",
            "Created val dataset with 564 image-mask pairs\n",
            "Finding valid image-mask pairs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checking image-mask pairs: 100%|██████████| 3762/3762 [00:00<00:00, 72604.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3762 valid image-mask pairs\n",
            "Created test dataset with 565 image-mask pairs\n",
            "Dataset split: Train=2633, Val=564, Test=565\n",
            "Creating model...\n",
            "Total parameters: 4,998,682\n",
            "Trainable parameters: 4,998,682\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 [Train]:  31%|███▏      | 103/329 [05:58<13:04,  3.47s/it, loss=0.6777, lr=0.000010]"
          ]
        }
      ],
      "source": [
        "# # -------------------------\n",
        "# # Main Function\n",
        "# # -------------------------\n",
        "# def main():\n",
        "#     # Setup directories\n",
        "#     OUTPUT_DIR = \"/kaggle/working/polygen_output\"\n",
        "#     CHECKPOINT_DIR = \"/kaggle/working/polygen_checkpoints\"\n",
        "#     os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "#     os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#     print(f\"Using device: {device}\")\n",
        "\n",
        "#     # Print GPU info if available\n",
        "#     if torch.cuda.is_available():\n",
        "#         print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "#         print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "#     # Set seeds for reproducibility\n",
        "#     torch.manual_seed(42)\n",
        "#     np.random.seed(42)\n",
        "#     random.seed(42)\n",
        "#     if torch.cuda.is_available():\n",
        "#         torch.cuda.manual_seed(42)\n",
        "#         torch.backends.cudnn.deterministic = True\n",
        "#         torch.backends.cudnn.benchmark = False\n",
        "\n",
        "#     print(\"Finding PolyGen2021 dataset in Kaggle environment...\")\n",
        "#     images_dir, masks_dir = find_dataset_paths()\n",
        "\n",
        "#     if not images_dir or not masks_dir:\n",
        "#         # Try with the known structure based on the error message\n",
        "#         images_dir = \"/kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images\"\n",
        "#         masks_dir = \"/kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks\"\n",
        "\n",
        "#     print(f\"Using images from: {images_dir}\")\n",
        "#     print(f\"Using masks from: {masks_dir}\")\n",
        "\n",
        "#     print(\"Creating data loaders...\")\n",
        "#     # Using batch size of 8 as requested with seed for reproducibility\n",
        "#     train_loader, val_loader, test_loader, split_sizes = get_data_loaders(\n",
        "#         images_dir, masks_dir, batch_size=8, img_size=256,\n",
        "#         split_ratio=(0.7, 0.15, 0.15), num_workers=4, seed=42\n",
        "#     )\n",
        "\n",
        "#     print(f\"Dataset split: Train={split_sizes[0]}, Val={split_sizes[1]}, Test={split_sizes[2]}\")\n",
        "\n",
        "#     print(\"Creating model...\")\n",
        "#     model = MKPEContourAwareMambaUNet(num_classes=2, d_model=128, d_state=32, num_mamba_layers=6)\n",
        "#     model.gradient_checkpointing = True\n",
        "\n",
        "#     total_params = sum(p.numel() for p in model.parameters())\n",
        "#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#     print(f\"Total parameters: {total_params:,}\")\n",
        "#     print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "#     print(\"Starting training...\")\n",
        "#     # Using 200 epochs with learning rate from GOOD_mkpe_x\n",
        "#     history, best_iou, best_dice, best_epoch = train_model(\n",
        "#         model, train_loader, val_loader,\n",
        "#         num_epochs=200,\n",
        "#         learning_rate=1.8e-4,  # Matching the learning rate from GOOD_mkpe_x\n",
        "#         device=device,\n",
        "#         checkpoint_dir=CHECKPOINT_DIR,\n",
        "#         patience=15,  # Early stopping patience\n",
        "#         scheduler_type='cosine'  # Using cosine scheduler\n",
        "#     )\n",
        "\n",
        "#     # Load best model for testing\n",
        "#     print(\"\\nLoading best model for testing...\")\n",
        "#     checkpoint = torch.load(os.path.join(CHECKPOINT_DIR, 'best_model_polygen.pth'))\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "#     print(\"Performing comprehensive model testing...\")\n",
        "#     test_results = test_model_comprehensive(model, test_loader, device, use_tta=True)\n",
        "\n",
        "#     # Plot ROC curve\n",
        "#     print(\"\\nGenerating ROC and PR curves...\")\n",
        "#     roc_auc, pr_auc = plot_roc_curve(\n",
        "#         test_results['all_probs'],\n",
        "#         test_results['all_gt'],\n",
        "#         save_path=os.path.join(OUTPUT_DIR, 'roc_pr_curves.png')\n",
        "#     )\n",
        "\n",
        "#     print(\"\\nGenerating visualizations...\")\n",
        "#     visualize_predictions(\n",
        "#         model, test_loader, device, num_samples=8,\n",
        "#         save_path=os.path.join(OUTPUT_DIR, 'prediction_visualization.png')\n",
        "#     )\n",
        "\n",
        "#     visualize_contours(\n",
        "#         model, test_loader, device, num_samples=5,\n",
        "#         save_path=os.path.join(OUTPUT_DIR, 'contour_visualization.png')\n",
        "#     )\n",
        "\n",
        "#     # Save detailed test metrics\n",
        "#     metrics_summary = {\n",
        "#         'overall': test_results['avg_metrics'],\n",
        "#         'by_size': test_results['metrics_by_size'],\n",
        "#         'roc_auc': roc_auc,\n",
        "#         'pr_auc': pr_auc,\n",
        "#         'best_model': {\n",
        "#             'epoch': best_epoch,\n",
        "#             'iou': best_iou,\n",
        "#             'dice': best_dice\n",
        "#         }\n",
        "#     }\n",
        "\n",
        "#     # Save metrics summary as JSON\n",
        "#     import json\n",
        "#     with open(os.path.join(OUTPUT_DIR, 'test_metrics.json'), 'w') as f:\n",
        "#         # Convert numpy values to Python native types for JSON serialization\n",
        "#         def convert_to_serializable(obj):\n",
        "#             if isinstance(obj, (np.integer, np.floating, np.bool_)):\n",
        "#                 return obj.item()\n",
        "#             elif isinstance(obj, np.ndarray):\n",
        "#                 return obj.tolist()\n",
        "#             return obj\n",
        "\n",
        "#         # Process the metrics dictionary to make it JSON serializable\n",
        "#         serializable_metrics = json.loads(\n",
        "#             json.dumps(metrics_summary, default=convert_to_serializable)\n",
        "#         )\n",
        "#         json.dump(serializable_metrics, f, indent=4)\n",
        "\n",
        "#     # Final summary\n",
        "#     print(\"\\nTraining and evaluation complete!\")\n",
        "#     print(f\"Best validation IoU: {best_iou:.4f} (Epoch {best_epoch+1})\")\n",
        "#     print(f\"Best validation Dice: {best_dice:.4f}\")\n",
        "#     print(f\"Final test IoU: {test_results['avg_metrics']['iou']:.4f}\")\n",
        "#     print(f\"Final test Dice: {test_results['avg_metrics']['dice']:.4f}\")\n",
        "#     print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "#     print(f\"All results saved to {OUTPUT_DIR}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         main()\n",
        "#     except KeyboardInterrupt:\n",
        "#         print(\"Training interrupted by user\")\n",
        "#     except Exception as e:\n",
        "#         import traceback\n",
        "#         print(f\"Error during training: {e}\")\n",
        "#         traceback.print_exc()\n",
        "\n",
        "#         # Try to save a backup of the model if an error occurs\n",
        "#         try:\n",
        "#             if 'model' in locals() and 'optimizer' in locals():\n",
        "#                 torch.save({\n",
        "#                     'model_state_dict': model.state_dict(),\n",
        "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                 }, '/kaggle/working/emergency_backup.pth')\n",
        "#                 print(\"Emergency backup saved to /kaggle/working/emergency_backup.pth\")\n",
        "#         except:\n",
        "#             print(\"Could not save emergency backup\")"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 3459850,
          "sourceId": 6047955,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}