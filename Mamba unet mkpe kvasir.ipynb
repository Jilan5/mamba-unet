{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T22:47:11.281073Z",
     "iopub.status.busy": "2025-05-26T22:47:11.280815Z",
     "iopub.status.idle": "2025-05-26T22:47:15.268724Z",
     "shell.execute_reply": "2025-05-26T22:47:15.267814Z",
     "shell.execute_reply.started": "2025-05-26T22:47:11.281049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gdown tqdm matplotlib pillow einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T22:47:15.270061Z",
     "iopub.status.busy": "2025-05-26T22:47:15.269764Z",
     "iopub.status.idle": "2025-05-26T22:47:22.778418Z",
     "shell.execute_reply": "2025-05-26T22:47:22.777663Z",
     "shell.execute_reply.started": "2025-05-26T22:47:15.270011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import glob\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T22:47:22.780587Z",
     "iopub.status.busy": "2025-05-26T22:47:22.780276Z",
     "iopub.status.idle": "2025-05-26T22:47:22.969182Z",
     "shell.execute_reply": "2025-05-26T22:47:22.968291Z",
     "shell.execute_reply.started": "2025-05-26T22:47:22.780570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Multi-Kernel Positional Embedding Module\n",
    "# -------------------------\n",
    "class MultiKernelPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=8):\n",
    "        super(MultiKernelPositionalEmbedding, self).__init__()\n",
    "        self.mid_channels = max(8, in_channels // reduction)\n",
    "        \n",
    "        # Multiple kernels of different sizes to capture multi-scale spatial information\n",
    "        self.conv3x3 = nn.Conv2d(in_channels, self.mid_channels, kernel_size=3, padding=1)\n",
    "        self.conv5x5 = nn.Conv2d(in_channels, self.mid_channels, kernel_size=5, padding=2)\n",
    "        self.conv7x7 = nn.Conv2d(in_channels, self.mid_channels, kernel_size=7, padding=3)\n",
    "        \n",
    "        # Position-sensitive attention\n",
    "        self.position_attention = nn.Sequential(\n",
    "            nn.Conv2d(self.mid_channels * 3, in_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract multi-scale features\n",
    "        feat_3x3 = self.conv3x3(x)\n",
    "        feat_5x5 = self.conv5x5(x)\n",
    "        feat_7x7 = self.conv7x7(x)\n",
    "        \n",
    "        # Concatenate multi-scale features\n",
    "        multi_scale_feat = torch.cat([feat_3x3, feat_5x5, feat_7x7], dim=1)\n",
    "        \n",
    "        # Generate position-sensitive attention map\n",
    "        attention_map = self.position_attention(multi_scale_feat)\n",
    "        \n",
    "        # Apply attention to input features\n",
    "        enhanced = x * attention_map\n",
    "        \n",
    "        return enhanced\n",
    "\n",
    "# -------------------------\n",
    "# Double Convolution with MKPE\n",
    "# -------------------------\n",
    "class DoubleConvWithMKPE(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.same_channels = in_channels == out_channels\n",
    "        \n",
    "        # Double convolution\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Multi-Kernel Positional Embedding\n",
    "        self.mkpe = MultiKernelPositionalEmbedding(out_channels)\n",
    "        \n",
    "        # Optional projection for residual connection\n",
    "        if not self.same_channels:\n",
    "            self.project = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        identity = x if self.same_channels else self.project(x)\n",
    "        \n",
    "        # Double convolution\n",
    "        x = self.double_conv(x)\n",
    "        \n",
    "        # Apply Multi-Kernel Positional Embedding\n",
    "        x = self.mkpe(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        x = x + identity\n",
    "        \n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# Dataset Download and Setup - Adapted for Kaggle\n",
    "# -------------------------\n",
    "def download_and_setup_dataset(force_download=False):\n",
    "    \"\"\"Download and properly set up Kvasir-SEG dataset\"\"\"\n",
    "    base_path = '/kaggle/working/datasets'\n",
    "    kvasir_path = os.path.join(base_path, 'kvasir-seg')\n",
    "    \n",
    "    # First check if dataset exists in Kaggle input directory\n",
    "    kaggle_input_path = '/kaggle/input'\n",
    "    for dirname, _, _ in os.walk(kaggle_input_path):\n",
    "        if 'kvasir-seg' in dirname.lower() and os.path.exists(os.path.join(dirname, 'images')):\n",
    "            print(f\"Found Kvasir-SEG dataset at {dirname}\")\n",
    "            return dirname\n",
    "\n",
    "    # Make sure base directory exists\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    # Check if dataset already exists in the expected directory structure\n",
    "    if os.path.exists(os.path.join(kvasir_path, 'images')) and \\\n",
    "       os.path.exists(os.path.join(kvasir_path, 'masks')) and \\\n",
    "       len(os.listdir(os.path.join(kvasir_path, 'images'))) > 0 and \\\n",
    "       not force_download:\n",
    "        print(\"Kvasir-SEG dataset already exists.\")\n",
    "        return kvasir_path\n",
    "\n",
    "    # Direct URL to the zip file\n",
    "    dataset_url = \"https://datasets.simula.no/downloads/kvasir-seg.zip\"\n",
    "    zip_path = os.path.join(base_path, 'kvasir-seg.zip')\n",
    "\n",
    "    # Download the dataset\n",
    "    print(\"Downloading Kvasir-SEG dataset...\")\n",
    "    try:\n",
    "        response = requests.get(dataset_url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024\n",
    "        progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                f.write(data)\n",
    "        progress_bar.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Download completed, file saved to {zip_path}\")\n",
    "\n",
    "    # Extract the dataset\n",
    "    print(\"Extracting dataset...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_path)\n",
    "        print(\"Extraction completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check the structure of extracted files\n",
    "    extracted_files = glob.glob(os.path.join(base_path, \"**\"), recursive=True)\n",
    "    print(\"Extracted file structure:\")\n",
    "    for file in extracted_files[:10]:  # Show only first 10 files\n",
    "        print(f\"  {file}\")\n",
    "    if len(extracted_files) > 10:\n",
    "        print(f\"  ... and {len(extracted_files)-10} more files\")\n",
    "\n",
    "    # Locate the images and masks directories\n",
    "    image_dirs = glob.glob(os.path.join(base_path, \"**/images\"), recursive=True)\n",
    "    mask_dirs = glob.glob(os.path.join(base_path, \"**/masks\"), recursive=True)\n",
    "\n",
    "    print(f\"Found image directories: {image_dirs}\")\n",
    "    print(f\"Found mask directories: {mask_dirs}\")\n",
    "\n",
    "    # Ensure proper directory structure\n",
    "    os.makedirs(os.path.join(kvasir_path, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(kvasir_path, 'masks'), exist_ok=True)\n",
    "\n",
    "    # Copy files to the expected location if needed\n",
    "    if image_dirs and mask_dirs:\n",
    "        src_image_dir = image_dirs[0]\n",
    "        src_mask_dir = mask_dirs[0]\n",
    "\n",
    "        if src_image_dir != os.path.join(kvasir_path, 'images'):\n",
    "            print(f\"Moving images from {src_image_dir} to {os.path.join(kvasir_path, 'images')}\")\n",
    "            for img_file in os.listdir(src_image_dir):\n",
    "                shutil.copy(\n",
    "                    os.path.join(src_image_dir, img_file),\n",
    "                    os.path.join(kvasir_path, 'images', img_file)\n",
    "                )\n",
    "\n",
    "        if src_mask_dir != os.path.join(kvasir_path, 'masks'):\n",
    "            print(f\"Moving masks from {src_mask_dir} to {os.path.join(kvasir_path, 'masks')}\")\n",
    "            for mask_file in os.listdir(src_mask_dir):\n",
    "                shutil.copy(\n",
    "                    os.path.join(src_mask_dir, mask_file),\n",
    "                    os.path.join(kvasir_path, 'masks', mask_file)\n",
    "                )\n",
    "\n",
    "    # Clean up\n",
    "    try:\n",
    "        os.remove(zip_path)\n",
    "        print(\"Removed zip file.\")\n",
    "    except:\n",
    "        print(\"Could not remove zip file.\")\n",
    "\n",
    "    # Verify the dataset is now properly set up\n",
    "    if os.path.exists(os.path.join(kvasir_path, 'images')) and \\\n",
    "       os.path.exists(os.path.join(kvasir_path, 'masks')) and \\\n",
    "       len(os.listdir(os.path.join(kvasir_path, 'images'))) > 0:\n",
    "        print(\"Dataset setup completed successfully.\")\n",
    "        print(f\"Found {len(os.listdir(os.path.join(kvasir_path, 'images')))} images and \"\n",
    "              f\"{len(os.listdir(os.path.join(kvasir_path, 'masks')))} masks.\")\n",
    "        return kvasir_path\n",
    "    else:\n",
    "        print(\"Dataset setup failed.\")\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "# Dataset class - No changes\n",
    "# -------------------------\n",
    "class KvasirSEGDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None, target_transform=None, augment=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.augment = augment and split == 'train'  # Only augment training data\n",
    "        self.img_dir = os.path.join(root_dir, 'images')\n",
    "        self.mask_dir = os.path.join(root_dir, 'masks')\n",
    "\n",
    "        # Verify directories exist\n",
    "        if not os.path.exists(self.img_dir):\n",
    "            raise ValueError(f\"Images directory not found: {self.img_dir}\")\n",
    "        if not os.path.exists(self.mask_dir):\n",
    "            raise ValueError(f\"Masks directory not found: {self.mask_dir}\")\n",
    "\n",
    "        # Get all image files\n",
    "        self.images = sorted([f for f in os.listdir(self.img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        if not self.images:\n",
    "            raise ValueError(f\"No images found in {self.img_dir}\")\n",
    "\n",
    "        # Print some sample image names for debugging\n",
    "        print(f\"Sample image names: {self.images[:5]}\")\n",
    "\n",
    "        # Split data into train/val/test (80/10/10 split)\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        indices = np.random.permutation(len(self.images))\n",
    "\n",
    "        if split == 'train':\n",
    "            self.images = [self.images[i] for i in indices[:int(0.8 * len(self.images))]]\n",
    "        elif split == 'val':\n",
    "            self.images = [self.images[i] for i in indices[int(0.8 * len(self.images)):int(0.9 * len(self.images))]]\n",
    "        else:  # test\n",
    "            self.images = [self.images[i] for i in indices[int(0.9 * len(self.images)):]]\n",
    "\n",
    "        print(f\"Created {split} dataset with {len(self.images)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        # Find corresponding mask\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "        mask_candidates = [\n",
    "            os.path.join(self.mask_dir, base_name + ext)\n",
    "            for ext in ['.jpg', '.png', '.jpeg', '.tif']\n",
    "        ]\n",
    "        mask_path = next((path for path in mask_candidates if os.path.exists(path)), None)\n",
    "\n",
    "        if not mask_path:\n",
    "            # Look for files that start with the same name\n",
    "            mask_files = os.listdir(self.mask_dir)\n",
    "            matches = [f for f in mask_files if f.startswith(base_name)]\n",
    "            if matches:\n",
    "                mask_path = os.path.join(self.mask_dir, matches[0])\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No mask found for image {img_name}\")\n",
    "\n",
    "        # Print paths for debugging (only for the first item)\n",
    "        if idx == 0:\n",
    "            print(f\"Image path: {img_path}\")\n",
    "            print(f\"Mask path: {mask_path}\")\n",
    "\n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.hflip(image)\n",
    "                mask = TF.hflip(mask)\n",
    "\n",
    "            # Random vertical flip\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.vflip(image)\n",
    "                mask = TF.vflip(mask)\n",
    "\n",
    "            # Random rotation\n",
    "            if random.random() > 0.5:\n",
    "                angle = random.choice([90, 180, 270])\n",
    "                fill = 0\n",
    "                image = TF.rotate(image, angle, fill=fill)\n",
    "                mask = TF.rotate(mask, angle, fill=fill)\n",
    "\n",
    "            # Color jitter (only for image)\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.adjust_brightness(image, random.uniform(0.8, 1.2))\n",
    "                image = TF.adjust_contrast(image, random.uniform(0.8, 1.2))\n",
    "                image = TF.adjust_saturation(image, random.uniform(0.8, 1.2))\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        else:\n",
    "            # Default transformation for masks\n",
    "            mask_array = np.array(mask)\n",
    "            mask_binary = (mask_array > 0).astype(np.int64)\n",
    "            mask = torch.from_numpy(mask_binary).long()  # Explicit cast to long\n",
    "\n",
    "        # For debugging: print data types and ranges (only for the first item)\n",
    "        if idx == 0:\n",
    "            print(f\"Image shape: {image.shape}, dtype: {image.dtype}, range: [{image.min()}, {image.max()}]\")\n",
    "            print(f\"Mask shape: {mask.shape}, dtype: {mask.dtype}, range: [{mask.min()}, {mask.max()}]\")\n",
    "\n",
    "        # Ensure mask is 2D (H,W) not 3D (1,H,W)\n",
    "        if mask.dim() == 3 and mask.size(0) == 1:\n",
    "            mask = mask.squeeze(0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# -------------------------\n",
    "# Enhanced Selective Scan - No changes\n",
    "# -------------------------\n",
    "def selective_scan(u, delta, A, B, C, D):\n",
    "    # Add numerical stability measures\n",
    "    A = torch.clamp(A, min=-5.0, max=5.0)\n",
    "\n",
    "    dA = torch.einsum('bld,dn->bldn', delta, A)\n",
    "    dB_u = torch.einsum('bld,bld,bln->bldn', delta, u, B)\n",
    "\n",
    "    dA_cumsum = torch.cat([dA[:, 1:], torch.zeros_like(dA[:, :1])], dim=1)\n",
    "    dA_cumsum = torch.flip(dA_cumsum, dims=[1])\n",
    "    dA_cumsum = torch.cumsum(dA_cumsum, dim=1)\n",
    "    dA_cumsum = torch.clamp(dA_cumsum, max=15.0)\n",
    "    dA_cumsum = torch.exp(dA_cumsum)\n",
    "    dA_cumsum = torch.flip(dA_cumsum, dims=[1])\n",
    "\n",
    "    x = dB_u * dA_cumsum\n",
    "    x = torch.cumsum(x, dim=1) / (dA_cumsum + 1e-6)\n",
    "\n",
    "    y = torch.einsum('bldn,bln->bld', x, C)\n",
    "    return y + u * D\n",
    "\n",
    "# -------------------------\n",
    "# Combined Loss Function - No changes\n",
    "# -------------------------\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # BCE Loss\n",
    "        bce = self.bce_loss(inputs, targets)\n",
    "\n",
    "        # Dice Loss\n",
    "        inputs_soft = F.softmax(inputs, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        # Calculate Dice loss manually\n",
    "        intersection = (inputs_soft * targets_one_hot).sum(dim=(2, 3))\n",
    "        cardinality = inputs_soft.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))\n",
    "\n",
    "        dice = (2. * intersection / (cardinality + 1e-6)).mean()\n",
    "        dice_loss = 1 - dice\n",
    "\n",
    "        # Combined loss\n",
    "        return self.bce_weight * bce + self.dice_weight * dice_loss\n",
    "\n",
    "# -------------------------\n",
    "# Improved MambaBlock - No changes\n",
    "# -------------------------\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.in_proj = nn.Linear(args.model_input_dims, args.model_internal_dim * 2, bias=False)\n",
    "        self.conv1d = nn.Conv1d(args.model_internal_dim, args.model_internal_dim, kernel_size=args.conv_kernel_size,\n",
    "                               padding=args.conv_kernel_size-1, groups=args.model_internal_dim)\n",
    "        self.x_proj = nn.Linear(args.model_internal_dim, args.delta_t_rank + args.model_states * 2, bias=False)\n",
    "        self.delta_proj = nn.Linear(args.delta_t_rank, args.model_internal_dim)\n",
    "\n",
    "        # Initialize A values\n",
    "        A_vals = torch.arange(1, args.model_states + 1).float() / args.model_states * 3\n",
    "        self.A_log = nn.Parameter(torch.log(repeat(A_vals, 'n -> d n', d=args.model_internal_dim)))\n",
    "        self.D = nn.Parameter(torch.ones(args.model_internal_dim))\n",
    "        self.out_proj = nn.Linear(args.model_internal_dim, args.model_input_dims, bias=args.dense_use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use gradient checkpointing for better memory efficiency during training\n",
    "        if self.training:\n",
    "            return torch.utils.checkpoint.checkpoint(self._forward, x, use_reentrant=False)\n",
    "        else:\n",
    "            return self._forward(x)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, l, d = x.shape\n",
    "        x_and_res = self.in_proj(x)\n",
    "        x1, res = x_and_res.chunk(2, dim=-1)\n",
    "\n",
    "        x1 = rearrange(x1, 'b l d -> b d l')\n",
    "        x1 = self.conv1d(x1)[..., :l]\n",
    "        x1 = rearrange(x1, 'b d l -> b l d')\n",
    "        x1 = F.silu(x1)\n",
    "\n",
    "        # Apply bounded values for more stability\n",
    "        A = -torch.exp(torch.clamp(self.A_log, min=-5, max=5))\n",
    "        D = self.D\n",
    "        x_dbl = self.x_proj(x1)\n",
    "        delta, B, C = torch.split(x_dbl, [self.args.delta_t_rank, self.args.model_states, self.args.model_states], dim=-1)\n",
    "        delta = F.softplus(self.delta_proj(delta))\n",
    "\n",
    "        y = selective_scan(x1, delta, A, B, C, D)\n",
    "        y = y * F.silu(res)\n",
    "        return self.out_proj(y)\n",
    "\n",
    "# -------------------------\n",
    "# Enhanced Residual Block - No changes\n",
    "# -------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(args.model_input_dims)\n",
    "        self.mixer = MambaBlock(args)\n",
    "        self.dropout = nn.Dropout(args.dropout_rate)\n",
    "        self.norm2 = nn.LayerNorm(args.model_input_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.mixer(x)\n",
    "        x = self.dropout(x)\n",
    "        x = residual + x\n",
    "        return self.norm2(x)\n",
    "\n",
    "# -------------------------\n",
    "# Improved Model Args - No changes\n",
    "# -------------------------\n",
    "class ModelArgs:\n",
    "    def __init__(self):\n",
    "        # Model dimensions\n",
    "        self.model_input_dims = 96\n",
    "        self.model_states = 96\n",
    "        self.projection_expand_factor = 2\n",
    "        self.conv_kernel_size = 4\n",
    "        self.conv_use_bias = False\n",
    "        self.dense_use_bias = False\n",
    "        self.layer_id = -1\n",
    "        self.seq_length = 256\n",
    "        self.num_layers = 4\n",
    "        self.dropout_rate = 0.2\n",
    "        self.use_lm_head = False\n",
    "        self.num_classes = 2  # Binary segmentation\n",
    "        self.final_activation = 'none'\n",
    "        self.model_internal_dim = self.projection_expand_factor * self.model_input_dims\n",
    "        self.delta_t_rank = math.ceil(self.model_input_dims / 16)\n",
    "\n",
    "# -------------------------\n",
    "# Mamba-UNet with Multi-Kernel Positional Embedding\n",
    "# -------------------------\n",
    "class MambaUNetWithMKPE(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder path with MKPE\n",
    "        self.encoder1 = DoubleConvWithMKPE(3, 64)  # Input: 3 RGB channels\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.encoder2 = DoubleConvWithMKPE(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.encoder3 = DoubleConvWithMKPE(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Mamba blocks in the bottleneck\n",
    "        self.mamba_blocks = nn.Sequential(*[ResidualBlock(args) for _ in range(args.num_layers)])\n",
    "\n",
    "        # Bridge between CNN and Mamba\n",
    "        self.bridge_down = nn.Conv2d(256, args.model_input_dims, kernel_size=1)\n",
    "        self.bridge_up = nn.Conv2d(args.model_input_dims, 256, kernel_size=1)\n",
    "        \n",
    "        # MKPE for bottleneck features\n",
    "        self.bottleneck_mkpe = MultiKernelPositionalEmbedding(256)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConvWithMKPE(256, 128)  # 256 = 128 (upconv) + 128 (skip)\n",
    "        self.deep_sup3 = nn.Conv2d(128, args.num_classes, kernel_size=1)\n",
    "        self.mkpe3 = MultiKernelPositionalEmbedding(128)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConvWithMKPE(128, 64)  # 128 = 64 (upconv) + 64 (skip)\n",
    "        self.deep_sup2 = nn.Conv2d(64, args.num_classes, kernel_size=1)\n",
    "        self.mkpe2 = MultiKernelPositionalEmbedding(64)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.decoder1 = DoubleConvWithMKPE(35, 32)  # 35 = 32 (upconv) + 3 (original input)\n",
    "        self.mkpe1 = MultiKernelPositionalEmbedding(32)\n",
    "\n",
    "        # Final layer\n",
    "        self.final_conv = nn.Conv2d(32, args.num_classes, kernel_size=1)\n",
    "        \n",
    "        # Output MKPE module\n",
    "        self.output_mkpe = MultiKernelPositionalEmbedding(args.num_classes, reduction=2)\n",
    "\n",
    "    def forward(self, x, return_deep=False):\n",
    "        # Save input for skip connection\n",
    "        input_x = x\n",
    "        \n",
    "        # Encoder path\n",
    "        enc1 = self.encoder1(x)      # 64 channels\n",
    "        enc1_pool = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.encoder2(enc1_pool)  # 128 channels\n",
    "        enc2_pool = self.pool2(enc2)\n",
    "\n",
    "        enc3 = self.encoder3(enc2_pool)  # 256 channels\n",
    "        enc3_pool = self.pool3(enc3)\n",
    "\n",
    "        # Bridge to Mamba\n",
    "        bridge_out = self.bridge_down(enc3_pool)\n",
    "\n",
    "        # Reshape for Mamba blocks\n",
    "        b, c, h, w = bridge_out.size()\n",
    "        mamba_input = bridge_out.permute(0, 2, 3, 1).reshape(b, h * w, c)\n",
    "\n",
    "        # Apply Mamba blocks\n",
    "        mamba_output = self.mamba_blocks(mamba_input)\n",
    "\n",
    "        # Reshape back to 2D\n",
    "        mamba_output = mamba_output.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
    "\n",
    "        # Bridge back to CNN\n",
    "        mamba_output = self.bridge_up(mamba_output)\n",
    "        \n",
    "        # Apply MKPE to bottleneck features\n",
    "        mamba_output = self.bottleneck_mkpe(mamba_output)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        dec3 = self.upconv3(mamba_output)\n",
    "        if dec3.shape[2:] != enc2.shape[2:]:\n",
    "            dec3 = F.interpolate(dec3, size=enc2.shape[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Concatenate and process\n",
    "        dec3 = torch.cat([dec3, enc2], dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec3 = self.mkpe3(dec3)  # Apply MKPE\n",
    "        deep_out3 = self.deep_sup3(dec3)\n",
    "\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        if dec2.shape[2:] != enc1.shape[2:]:\n",
    "            dec2 = F.interpolate(dec2, size=enc1.shape[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Concatenate and process\n",
    "        dec2 = torch.cat([dec2, enc1], dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec2 = self.mkpe2(dec2)  # Apply MKPE\n",
    "        deep_out2 = self.deep_sup2(dec2)\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        if dec1.shape[2:] != input_x.shape[2:]:\n",
    "            dec1 = F.interpolate(dec1, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Concatenate and process\n",
    "        dec1 = torch.cat([dec1, input_x], dim=1)  # Skip connection to original input\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        dec1 = self.mkpe1(dec1)  # Apply MKPE\n",
    "\n",
    "        # Final layer with MKPE\n",
    "        out = self.final_conv(dec1)\n",
    "        out = self.output_mkpe(out)  # Output MKPE\n",
    "\n",
    "        if return_deep:\n",
    "            # Return main output and deep supervision outputs\n",
    "            deep_out2 = F.interpolate(deep_out2, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n",
    "            deep_out3 = F.interpolate(deep_out3, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n",
    "            return out, deep_out2, deep_out3\n",
    "\n",
    "        return out\n",
    "\n",
    "# -------------------------\n",
    "# Deep Supervision Loss - No changes\n",
    "# -------------------------\n",
    "class DeepSupervisionLoss(nn.Module):\n",
    "    def __init__(self, main_weight=0.6, deep2_weight=0.2, deep3_weight=0.2):\n",
    "        super(DeepSupervisionLoss, self).__init__()\n",
    "        self.main_weight = main_weight\n",
    "        self.deep2_weight = deep2_weight\n",
    "        self.deep3_weight = deep3_weight\n",
    "        self.criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        main_out, deep2, deep3 = outputs\n",
    "\n",
    "        loss_main = self.criterion(main_out, target)\n",
    "        loss_deep2 = self.criterion(deep2, target)\n",
    "        loss_deep3 = self.criterion(deep3, target)\n",
    "\n",
    "        total_loss = (\n",
    "            self.main_weight * loss_main +\n",
    "            self.deep2_weight * loss_deep2 +\n",
    "            self.deep3_weight * loss_deep3\n",
    "        )\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation Metrics - No changes\n",
    "# -------------------------\n",
    "def calculate_iou(pred_mask, gt_mask):\n",
    "    \"\"\"Calculate IoU for binary segmentation\"\"\"\n",
    "    pred_mask = (pred_mask > 0).cpu().numpy().astype(bool)\n",
    "    gt_mask = (gt_mask > 0).cpu().numpy().astype(bool)\n",
    "\n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "\n",
    "    if union == 0:\n",
    "        return 1.0  # If both masks are empty, IoU is 1\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "def calculate_dice(pred_mask, gt_mask):\n",
    "    \"\"\"Calculate Dice coefficient\"\"\"\n",
    "    pred_mask = (pred_mask > 0).cpu().numpy().astype(bool)\n",
    "    gt_mask = (gt_mask > 0).cpu().numpy().astype(bool)\n",
    "\n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    sum_areas = pred_mask.sum() + gt_mask.sum()\n",
    "\n",
    "    if sum_areas == 0:\n",
    "        return 1.0  # If both masks are empty, Dice is 1\n",
    "\n",
    "    return 2.0 * intersection / sum_areas\n",
    "\n",
    "# -------------------------\n",
    "# Plot training progress - Adapted for Kaggle\n",
    "# -------------------------\n",
    "def plot_training_progress(history, epoch):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['train_iou'], label='Train IoU')\n",
    "    plt.plot(history['val_iou'], label='Val IoU')\n",
    "    plt.title('IoU')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['train_dice'], label='Train Dice')\n",
    "    plt.plot(history['val_dice'], label='Val Dice')\n",
    "    plt.title('Dice')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"/kaggle/working/training_progress_epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Enhanced Training Function - No changes\n",
    "# -------------------------\n",
    "def train_one_epoch_enhanced(model, dataloader, optimizer, criterion, device, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    running_dice = 0.0\n",
    "    sample_count = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "\n",
    "    for i, (images, masks) in enumerate(pbar):\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Check data shape for the first batch\n",
    "        if i == 0:\n",
    "            print(f\"Training batch - Images: {images.shape}, Masks: {masks.shape}\")\n",
    "            print(f\"Masks unique values: {torch.unique(masks)}\")\n",
    "\n",
    "        # Forward pass with deep supervision\n",
    "        outputs = model(images, return_deep=True)\n",
    "\n",
    "        # Calculate loss with deep supervision\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Optional gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step scheduler if provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Get main output for metrics calculation\n",
    "        main_output = outputs[0]\n",
    "\n",
    "        # Calculate metrics\n",
    "        batch_size = images.size(0)\n",
    "        preds = torch.argmax(main_output, dim=1)\n",
    "\n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * batch_size\n",
    "\n",
    "        # Calculate metrics per image\n",
    "        batch_iou = 0\n",
    "        batch_dice = 0\n",
    "        for j in range(batch_size):\n",
    "            iou = calculate_iou(preds[j], masks[j])\n",
    "            dice = calculate_dice(preds[j], masks[j])\n",
    "            batch_iou += iou\n",
    "            batch_dice += dice\n",
    "\n",
    "        running_iou += batch_iou\n",
    "        running_dice += batch_dice\n",
    "        sample_count += batch_size\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'iou': batch_iou / batch_size,\n",
    "            'dice': batch_dice / batch_size\n",
    "        })\n",
    "\n",
    "        # Clear some GPU memory if needed\n",
    "        del outputs, loss, preds\n",
    "        if i % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / sample_count\n",
    "    epoch_iou = running_iou / sample_count\n",
    "    epoch_dice = running_dice / sample_count\n",
    "\n",
    "    return epoch_loss, epoch_iou, epoch_dice\n",
    "\n",
    "# -------------------------\n",
    "# Validation Function - No changes\n",
    "# -------------------------\n",
    "def validate_enhanced(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    running_dice = 0.0\n",
    "    sample_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc='Validation'):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Forward pass with deep supervision\n",
    "            outputs = model(images, return_deep=True)\n",
    "\n",
    "            # Calculate loss with deep supervision\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # Get main output for metrics calculation\n",
    "            main_output = outputs[0]\n",
    "\n",
    "            # Calculate metrics\n",
    "            batch_size = images.size(0)\n",
    "            preds = torch.argmax(main_output, dim=1)\n",
    "\n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * batch_size\n",
    "\n",
    "            # Calculate metrics per image\n",
    "            for j in range(batch_size):\n",
    "                iou = calculate_iou(preds[j], masks[j])\n",
    "                dice = calculate_dice(preds[j], masks[j])\n",
    "                running_iou += iou\n",
    "                running_dice += dice\n",
    "\n",
    "            sample_count += batch_size\n",
    "\n",
    "    # Calculate statistics\n",
    "    val_loss = running_loss / sample_count\n",
    "    val_iou = running_iou / sample_count\n",
    "    val_dice = running_dice / sample_count\n",
    "\n",
    "    return val_loss, val_iou, val_dice\n",
    "\n",
    "# -------------------------\n",
    "# Visualization Function - Adapted for Kaggle\n",
    "# -------------------------\n",
    "def visualize_results(model, dataloader, device, num_samples=3):\n",
    "    model.eval()\n",
    "\n",
    "    # Get a batch of data\n",
    "    images, masks = next(iter(dataloader))\n",
    "    images = images[:num_samples].to(device)\n",
    "    masks = masks[:num_samples].to(device)\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    # Denormalize images for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    images = images * std + mean\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Display original image\n",
    "        axes[i, 0].imshow(images[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[i, 0].set_title(\"Original Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        # Display ground truth mask\n",
    "        axes[i, 1].imshow(masks[i].cpu().numpy(), cmap=\"gray\")\n",
    "        axes[i, 1].set_title(\"Ground Truth\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        # Display predicted mask\n",
    "        axes[i, 2].imshow(predictions[i].cpu().numpy(), cmap=\"gray\")\n",
    "        axes[i, 2].set_title(\"Prediction\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/kaggle/working/mamba_segmentation_with_mkpe_results.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T22:47:22.970453Z",
     "iopub.status.busy": "2025-05-26T22:47:22.970128Z",
     "iopub.status.idle": "2025-05-27T03:28:43.396170Z",
     "shell.execute_reply": "2025-05-27T03:28:43.395265Z",
     "shell.execute_reply.started": "2025-05-26T22:47:22.970426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Main Function - Adapted for Kaggle\n",
    "# -------------------------\n",
    "def main():\n",
    "    print(\"Starting Mamba-UNet with Multi-Kernel Positional Embedding...\")\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "    # Download and set up the dataset\n",
    "    kvasir_path = download_and_setup_dataset(force_download=False)\n",
    "\n",
    "    if not kvasir_path:\n",
    "        print(\"Dataset setup failed. Exiting...\")\n",
    "        return\n",
    "\n",
    "    # Define transformations\n",
    "    transform = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    target_transform = T.Compose([\n",
    "        T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "        T.ToTensor(),\n",
    "        lambda x: (x > 0.5).long()\n",
    "    ])\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    try:\n",
    "        train_dataset = KvasirSEGDataset(\n",
    "            kvasir_path,\n",
    "            split='train',\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "            augment=True\n",
    "        )\n",
    "\n",
    "        val_dataset = KvasirSEGDataset(\n",
    "            kvasir_path,\n",
    "            split='val',\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "            augment=False\n",
    "        )\n",
    "\n",
    "        # Use batch size of 4 as requested\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=4,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=4,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "\n",
    "        print(\"Data loaders created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating datasets: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # Initialize model args and create model with MKPE\n",
    "    args = ModelArgs()\n",
    "    model = MambaUNetWithMKPE(args).to(device)\n",
    "    print(\"Mamba-UNet model created with Multi-Kernel Positional Embedding.\")\n",
    "\n",
    "    # Define enhanced loss function and optimizer\n",
    "    criterion = DeepSupervisionLoss(main_weight=0.6, deep2_weight=0.2, deep3_weight=0.2)\n",
    "    \n",
    "    # Slightly different learning rate for MKPE model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Add learning rate scheduler with warm restarts\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=10,  # Restart every 10 epochs\n",
    "        T_mult=2,  # Double period after each restart\n",
    "        eta_min=1e-6,\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    best_iou = 0.0\n",
    "    patience_counter = 0\n",
    "    max_patience = 20  # Early stopping after 20 epochs without improvement\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [], 'train_iou': [], 'train_dice': [],\n",
    "        'val_loss': [], 'val_iou': [], 'val_dice': []\n",
    "    }\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            # Train with enhanced functions\n",
    "            train_loss, train_iou, train_dice = train_one_epoch_enhanced(\n",
    "                model, train_loader, optimizer, criterion, device, scheduler\n",
    "            )\n",
    "\n",
    "            # Validate\n",
    "            val_loss, val_iou, val_dice = validate_enhanced(\n",
    "                model, val_loader, criterion, device\n",
    "            )\n",
    "\n",
    "            # Save history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_iou'].append(train_iou)\n",
    "            history['train_dice'].append(train_dice)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_iou'].append(val_iou)\n",
    "            history['val_dice'].append(val_dice)\n",
    "\n",
    "            # Print epoch results\n",
    "            print(f\"Train - Loss: {train_loss:.4f}, IoU: {train_iou:.4f}, Dice: {train_dice:.4f}\")\n",
    "            print(f\"Val   - Loss: {val_loss:.4f}, IoU: {val_iou:.4f}, Dice: {val_dice:.4f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_iou > best_iou:\n",
    "                best_iou = val_iou\n",
    "                torch.save(model.state_dict(), \"/kaggle/working/best_mamba_unet_with_mkpe.pth\")\n",
    "                print(f\"Model saved with IoU: {best_iou:.4f}\")\n",
    "                patience_counter = 0  # Reset patience counter\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Save checkpoint every 10 epochs for safety\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_iou': best_iou,\n",
    "                    'history': history,\n",
    "                }, f\"/kaggle/working/checkpoint_mkpe_epoch_{epoch+1}.pth\")\n",
    "\n",
    "                # Plot and save training progress\n",
    "                plot_training_progress(history, epoch+1)\n",
    "\n",
    "            # Early stopping\n",
    "            if patience_counter >= max_patience:\n",
    "                print(f\"Early stopping after {max_patience} epochs without improvement\")\n",
    "                break\n",
    "\n",
    "            # Clear GPU cache between epochs\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "        # Save checkpoint on error\n",
    "        torch.save({\n",
    "            'epoch': epoch if 'epoch' in locals() else 0,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if 'scheduler' in locals() else None,\n",
    "            'best_iou': best_iou if 'best_iou' in locals() else 0,\n",
    "            'history': history,\n",
    "        }, \"/kaggle/working/error_checkpoint_mkpe.pth\")\n",
    "\n",
    "    # Load best model for evaluation\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(\"/kaggle/working/best_mamba_unet_with_mkpe.pth\"))\n",
    "        print(\"Loaded best model for evaluation\")\n",
    "    except:\n",
    "        print(\"Could not load best model, using current model\")\n",
    "\n",
    "    # Visualize results\n",
    "    visualize_results(model, val_loader, device)\n",
    "\n",
    "    print(\"Training and evaluation completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
