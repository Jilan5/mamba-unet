{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_and_introduction"
      },
      "source": [
        "# Mamba-UNet with Feature Fusion and Depthwise Convolution for Polyp Segmentation\n",
        "\n",
        "This notebook implements an enhanced Mamba-UNet architecture with Feature Fusion using Depthwise Convolution for medical image segmentation on the Kvasir-SEG polyp dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install gdown tqdm matplotlib pillow einops requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from einops import rearrange, repeat\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import glob\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_download_section"
      },
      "source": [
        "## Dataset Download and Setup\n",
        "\n",
        "The following function will download and set up the Kvasir-SEG dataset if it doesn't already exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_download"
      },
      "outputs": [],
      "source": [
        "def download_and_setup_dataset(force_download=False):\n",
        "    \"\"\"Download and properly set up Kvasir-SEG dataset\"\"\"\n",
        "    base_path = './datasets'\n",
        "    kvasir_path = os.path.join(base_path, 'kvasir-seg')\n",
        "\n",
        "    # Make sure base directory exists\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "\n",
        "    # Check if dataset already exists in the expected directory structure\n",
        "    if os.path.exists(os.path.join(kvasir_path, 'images')) and \\\n",
        "       os.path.exists(os.path.join(kvasir_path, 'masks')) and \\\n",
        "       len(os.listdir(os.path.join(kvasir_path, 'images'))) > 0 and \\\n",
        "       not force_download:\n",
        "        print(\"Kvasir-SEG dataset already exists.\")\n",
        "        return kvasir_path\n",
        "\n",
        "    # Direct URL to the zip file\n",
        "    dataset_url = \"https://datasets.simula.no/downloads/kvasir-seg.zip\"\n",
        "    zip_path = os.path.join(base_path, 'kvasir-seg.zip')\n",
        "\n",
        "    # Download the dataset\n",
        "    print(\"Downloading Kvasir-SEG dataset...\")\n",
        "    try:\n",
        "        response = requests.get(dataset_url, stream=True)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        block_size = 1024\n",
        "        progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
        "\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            for data in response.iter_content(block_size):\n",
        "                progress_bar.update(len(data))\n",
        "                f.write(data)\n",
        "        progress_bar.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Download completed, file saved to {zip_path}\")\n",
        "\n",
        "    # Extract the dataset\n",
        "    print(\"Extracting dataset...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(base_path)\n",
        "        print(\"Extraction completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Check the structure of extracted files\n",
        "    extracted_files = glob.glob(os.path.join(base_path, \"**\"), recursive=True)\n",
        "    print(\"Extracted file structure:\")\n",
        "    for file in extracted_files[:10]:  # Show only first 10 files\n",
        "        print(f\"  {file}\")\n",
        "    if len(extracted_files) > 10:\n",
        "        print(f\"  ... and {len(extracted_files)-10} more files\")\n",
        "\n",
        "    # Locate the images and masks directories\n",
        "    image_dirs = glob.glob(os.path.join(base_path, \"**/images\"), recursive=True)\n",
        "    mask_dirs = glob.glob(os.path.join(base_path, \"**/masks\"), recursive=True)\n",
        "\n",
        "    print(f\"Found image directories: {image_dirs}\")\n",
        "    print(f\"Found mask directories: {mask_dirs}\")\n",
        "\n",
        "    # Ensure proper directory structure\n",
        "    os.makedirs(os.path.join(kvasir_path, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(kvasir_path, 'masks'), exist_ok=True)\n",
        "\n",
        "    # Copy files to the expected location if needed\n",
        "    if image_dirs and mask_dirs:\n",
        "        src_image_dir = image_dirs[0]\n",
        "        src_mask_dir = mask_dirs[0]\n",
        "\n",
        "        if src_image_dir != os.path.join(kvasir_path, 'images'):\n",
        "            print(f\"Moving images from {src_image_dir} to {os.path.join(kvasir_path, 'images')}\")\n",
        "            for img_file in os.listdir(src_image_dir):\n",
        "                shutil.copy(\n",
        "                    os.path.join(src_image_dir, img_file),\n",
        "                    os.path.join(kvasir_path, 'images', img_file)\n",
        "                )\n",
        "\n",
        "        if src_mask_dir != os.path.join(kvasir_path, 'masks'):\n",
        "            print(f\"Moving masks from {src_mask_dir} to {os.path.join(kvasir_path, 'masks')}\")\n",
        "            for mask_file in os.listdir(src_mask_dir):\n",
        "                shutil.copy(\n",
        "                    os.path.join(src_mask_dir, mask_file),\n",
        "                    os.path.join(kvasir_path, 'masks', mask_file)\n",
        "                )\n",
        "\n",
        "    # Clean up\n",
        "    try:\n",
        "        os.remove(zip_path)\n",
        "        print(\"Removed zip file.\")\n",
        "    except:\n",
        "        print(\"Could not remove zip file.\")\n",
        "\n",
        "    # Verify the dataset is now properly set up\n",
        "    if os.path.exists(os.path.join(kvasir_path, 'images')) and \\\n",
        "       os.path.exists(os.path.join(kvasir_path, 'masks')) and \\\n",
        "       len(os.listdir(os.path.join(kvasir_path, 'images'))) > 0:\n",
        "        print(\"Dataset setup completed successfully.\")\n",
        "        print(f\"Found {len(os.listdir(os.path.join(kvasir_path, 'images')))} images and \"\n",
        "              f\"{len(os.listdir(os.path.join(kvasir_path, 'masks')))} masks.\")\n",
        "        return kvasir_path\n",
        "    else:\n",
        "        print(\"Dataset setup failed.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_class_section"
      },
      "source": [
        "## Dataset Class with Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_class"
      },
      "outputs": [],
      "source": [
        "class KvasirSEGDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None, target_transform=None, augment=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.augment = augment and split == 'train'  # Only augment training data\n",
        "        self.img_dir = os.path.join(root_dir, 'images')\n",
        "        self.mask_dir = os.path.join(root_dir, 'masks')\n",
        "\n",
        "        # Verify directories exist\n",
        "        if not os.path.exists(self.img_dir):\n",
        "            raise ValueError(f\"Images directory not found: {self.img_dir}\")\n",
        "        if not os.path.exists(self.mask_dir):\n",
        "            raise ValueError(f\"Masks directory not found: {self.mask_dir}\")\n",
        "\n",
        "        # Get all image files\n",
        "        self.images = sorted([f for f in os.listdir(self.img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "        if not self.images:\n",
        "            raise ValueError(f\"No images found in {self.img_dir}\")\n",
        "\n",
        "        # Print some sample image names for debugging\n",
        "        print(f\"Sample image names: {self.images[:5]}\")\n",
        "\n",
        "        # Split data into train/val/test (80/10/10 split)\n",
        "        np.random.seed(42)  # For reproducibility\n",
        "        indices = np.random.permutation(len(self.images))\n",
        "\n",
        "        if split == 'train':\n",
        "            self.images = [self.images[i] for i in indices[:int(0.8 * len(self.images))]]\n",
        "        elif split == 'val':\n",
        "            self.images = [self.images[i] for i in indices[int(0.8 * len(self.images)):int(0.9 * len(self.images))]]\n",
        "        else:  # test\n",
        "            self.images = [self.images[i] for i in indices[int(0.9 * len(self.images)):]]\n",
        "\n",
        "        print(f\"Created {split} dataset with {len(self.images)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # Find corresponding mask\n",
        "        base_name = os.path.splitext(img_name)[0]\n",
        "        mask_candidates = [\n",
        "            os.path.join(self.mask_dir, base_name + ext)\n",
        "            for ext in ['.jpg', '.png', '.jpeg', '.tif']\n",
        "        ]\n",
        "        mask_path = next((path for path in mask_candidates if os.path.exists(path)), None)\n",
        "\n",
        "        if not mask_path:\n",
        "            # Look for files that start with the same name\n",
        "            mask_files = os.listdir(self.mask_dir)\n",
        "            matches = [f for f in mask_files if f.startswith(base_name)]\n",
        "            if matches:\n",
        "                mask_path = os.path.join(self.mask_dir, matches[0])\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"No mask found for image {img_name}\")\n",
        "\n",
        "        # Print paths for debugging (only for the first item)\n",
        "        if idx == 0:\n",
        "            print(f\"Image path: {img_path}\")\n",
        "            print(f\"Mask path: {mask_path}\")\n",
        "\n",
        "        # Load image and mask\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        # Apply augmentation if enabled\n",
        "        if self.augment:\n",
        "            # Random horizontal flip\n",
        "            if random.random() > 0.5:\n",
        "                image = TF.hflip(image)\n",
        "                mask = TF.hflip(mask)\n",
        "\n",
        "            # Random vertical flip\n",
        "            if random.random() > 0.5:\n",
        "                image = TF.vflip(image)\n",
        "                mask = TF.vflip(mask)\n",
        "\n",
        "            # Random rotation\n",
        "            if random.random() > 0.5:\n",
        "                angle = random.choice([90, 180, 270])\n",
        "                fill = 0\n",
        "                image = TF.rotate(image, angle, fill=fill)\n",
        "                mask = TF.rotate(mask, angle, fill=fill)\n",
        "\n",
        "            # Color jitter (only for image)\n",
        "            if random.random() > 0.5:\n",
        "                image = TF.adjust_brightness(image, random.uniform(0.8, 1.2))\n",
        "                image = TF.adjust_contrast(image, random.uniform(0.8, 1.2))\n",
        "                image = TF.adjust_saturation(image, random.uniform(0.8, 1.2))\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "        else:\n",
        "            # Default transformation for masks\n",
        "            mask_array = np.array(mask)\n",
        "            mask_binary = (mask_array > 0).astype(np.int64)\n",
        "            mask = torch.from_numpy(mask_binary).long()  # Explicit cast to long\n",
        "\n",
        "        # For debugging: print data types and ranges (only for the first item)\n",
        "        if idx == 0:\n",
        "            print(f\"Image shape: {image.shape}, dtype: {image.dtype}, range: [{image.min()}, {image.max()}]\")\n",
        "            print(f\"Mask shape: {mask.shape}, dtype: {mask.dtype}, range: [{mask.min()}, {mask.max()}]\")\n",
        "\n",
        "        # Ensure mask is 2D (H,W) not 3D (1,H,W)\n",
        "        if mask.dim() == 3 and mask.size(0) == 1:\n",
        "            mask = mask.squeeze(0)\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "selective_scan_section"
      },
      "source": [
        "## Mamba Components: Selective Scan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "selective_scan"
      },
      "outputs": [],
      "source": [
        "def selective_scan(u, delta, A, B, C, D):\n",
        "    # Add numerical stability measures\n",
        "    # Clamp A values to prevent extreme values\n",
        "    A = torch.clamp(A, min=-5.0, max=5.0)\n",
        "\n",
        "    dA = torch.einsum('bld,dn->bldn', delta, A)\n",
        "    dB_u = torch.einsum('bld,bld,bln->bldn', delta, u, B)\n",
        "\n",
        "    dA_cumsum = torch.cat([dA[:, 1:], torch.zeros_like(dA[:, :1])], dim=1)\n",
        "    dA_cumsum = torch.flip(dA_cumsum, dims=[1])\n",
        "    dA_cumsum = torch.cumsum(dA_cumsum, dim=1)\n",
        "    # Prevent overflow in exponential\n",
        "    dA_cumsum = torch.clamp(dA_cumsum, max=15.0)\n",
        "    dA_cumsum = torch.exp(dA_cumsum)\n",
        "    dA_cumsum = torch.flip(dA_cumsum, dims=[1])\n",
        "\n",
        "    x = dB_u * dA_cumsum\n",
        "    x = torch.cumsum(x, dim=1) / (dA_cumsum + 1e-6)\n",
        "\n",
        "    y = torch.einsum('bldn,bln->bld', x, C)\n",
        "    return y + u * D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loss_functions_section"
      },
      "source": [
        "## Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loss_functions"
      },
      "outputs": [],
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "        self.bce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # BCE Loss\n",
        "        bce = self.bce_loss(inputs, targets)\n",
        "\n",
        "        # Dice Loss\n",
        "        inputs_soft = F.softmax(inputs, dim=1)\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n",
        "\n",
        "        # Calculate Dice loss manually\n",
        "        intersection = (inputs_soft * targets_one_hot).sum(dim=(2, 3))\n",
        "        cardinality = inputs_soft.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))\n",
        "\n",
        "        dice = (2. * intersection / (cardinality + 1e-6)).mean()\n",
        "        dice_loss = 1 - dice\n",
        "\n",
        "        # Combined loss\n",
        "        return self.bce_weight * bce + self.dice_weight * dice_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mamba_blocks_section"
      },
      "source": [
        "## Mamba and Residual Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mamba_blocks"
      },
      "outputs": [],
      "source": [
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.in_proj = nn.Linear(args.model_input_dims, args.model_internal_dim * 2, bias=False)\n",
        "        self.conv1d = nn.Conv1d(args.model_internal_dim, args.model_internal_dim, kernel_size=args.conv_kernel_size,\n",
        "                               padding=args.conv_kernel_size-1, groups=args.model_internal_dim)\n",
        "        self.x_proj = nn.Linear(args.model_internal_dim, args.delta_t_rank + args.model_states * 2, bias=False)\n",
        "        self.delta_proj = nn.Linear(args.delta_t_rank, args.model_internal_dim)\n",
        "\n",
        "        # Initialize A values more carefully for better gradient flow\n",
        "        A_vals = torch.arange(1, args.model_states + 1).float() / args.model_states * 3\n",
        "        self.A_log = nn.Parameter(torch.log(repeat(A_vals, 'n -> d n', d=args.model_internal_dim)))\n",
        "        self.D = nn.Parameter(torch.ones(args.model_internal_dim))\n",
        "        self.out_proj = nn.Linear(args.model_internal_dim, args.model_input_dims, bias=args.dense_use_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use gradient checkpointing for better memory efficiency during training\n",
        "        if self.training:\n",
        "            return torch.utils.checkpoint.checkpoint(self._forward, x, use_reentrant=False)\n",
        "        else:\n",
        "            return self._forward(x)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        b, l, d = x.shape\n",
        "        x_and_res = self.in_proj(x)\n",
        "        x1, res = x_and_res.chunk(2, dim=-1)\n",
        "\n",
        "        x1 = rearrange(x1, 'b l d -> b d l')\n",
        "        x1 = self.conv1d(x1)[..., :l]\n",
        "        x1 = rearrange(x1, 'b d l -> b l d')\n",
        "        x1 = F.silu(x1)\n",
        "\n",
        "        # Apply bounded values for more stability\n",
        "        A = -torch.exp(torch.clamp(self.A_log, min=-5, max=5))\n",
        "        D = self.D\n",
        "        x_dbl = self.x_proj(x1)\n",
        "        delta, B, C = torch.split(x_dbl, [self.args.delta_t_rank, self.args.model_states, self.args.model_states], dim=-1)\n",
        "        delta = F.softplus(self.delta_proj(delta))\n",
        "\n",
        "        y = selective_scan(x1, delta, A, B, C, D)\n",
        "        y = y * F.silu(res)\n",
        "        return self.out_proj(y)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(args.model_input_dims)\n",
        "        self.mixer = MambaBlock(args)\n",
        "        self.dropout = nn.Dropout(args.dropout_rate)\n",
        "        self.norm2 = nn.LayerNorm(args.model_input_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # More transformer-like architecture with normalization\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.mixer(x)\n",
        "        x = self.dropout(x)\n",
        "        x = residual + x\n",
        "        return self.norm2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_fusion_section"
      },
      "source": [
        "## Feature Fusion with Depthwise Convolution Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_fusion"
      },
      "outputs": [],
      "source": [
        "class DepthwiseFeatureFusion(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=None):\n",
        "        super().__init__()\n",
        "        if out_channels is None:\n",
        "            out_channels = in_channels\n",
        "            \n",
        "        # Depthwise convolution\n",
        "        self.dw_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels, bias=False)\n",
        "        # Pointwise convolution to mix the features\n",
        "        self.pw_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.norm = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        \n",
        "        # Channel attention\n",
        "        self.channel_attention = ChannelAttention(out_channels)\n",
        "    \n",
        "    def forward(self, x1, x2=None):\n",
        "        if x2 is not None:\n",
        "            # Ensure dimensions match\n",
        "            if x1.shape[2:] != x2.shape[2:]:\n",
        "                x2 = F.interpolate(x2, size=x1.shape[2:], mode='bilinear', align_corners=True)\n",
        "            \n",
        "            # Channel-wise feature fusion\n",
        "            if x1.shape[1] != x2.shape[1]:\n",
        "                x2 = self.pw_conv(x2)  # Adjust channels if needed\n",
        "                x2 = self.norm(x2)\n",
        "                x2 = self.activation(x2)\n",
        "                \n",
        "            # Fuse features\n",
        "            fused = x1 + x2\n",
        "        else:\n",
        "            fused = x1\n",
        "        \n",
        "        # Apply depthwise convolution followed by pointwise\n",
        "        fused = self.dw_conv(fused)\n",
        "        fused = self.pw_conv(fused)\n",
        "        fused = self.norm(fused)\n",
        "        fused = self.activation(fused)\n",
        "        \n",
        "        # Apply channel attention\n",
        "        fused = self.channel_attention(fused)\n",
        "        \n",
        "        return fused\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction_ratio=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        \n",
        "        # Shared MLP\n",
        "        reduced_channels = max(8, channels // reduction_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(reduced_channels, channels, kernel_size=1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        avg_pool = self.avg_pool(x)\n",
        "        max_pool = self.max_pool(x)\n",
        "        \n",
        "        avg_out = self.mlp(avg_pool)\n",
        "        max_out = self.mlp(max_pool)\n",
        "        \n",
        "        channel_attention = self.sigmoid(avg_out + max_out)\n",
        "        return x * channel_attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conv_blocks_section"
      },
      "source": [
        "## Enhanced Convolution Blocks with Residual Connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "conv_blocks"
      },
      "outputs": [],
      "source": [
        "class DoubleConvWithResidual(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.same_channels = in_channels == out_channels\n",
        "\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Optional projection for residual connection when channel sizes differ\n",
        "        if not self.same_channels:\n",
        "            self.project = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x if self.same_channels else self.project(x)\n",
        "        x = self.double_conv(x)\n",
        "        x = x + identity  # Residual connection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_args_section"
      },
      "source": [
        "## Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_args"
      },
      "outputs": [],
      "source": [
        "class ModelArgs:\n",
        "    def __init__(self):\n",
        "        # Model dimensions\n",
        "        self.model_input_dims = 96\n",
        "        self.model_states = 96\n",
        "        self.projection_expand_factor = 2\n",
        "        self.conv_kernel_size = 4\n",
        "        self.conv_use_bias = False\n",
        "        self.dense_use_bias = False\n",
        "        self.layer_id = -1\n",
        "        self.seq_length = 256\n",
        "        self.num_layers = 4\n",
        "        self.dropout_rate = 0.2\n",
        "        self.use_lm_head = False\n",
        "        self.num_classes = 2  # Binary segmentation\n",
        "        self.final_activation = 'none'\n",
        "        self.model_internal_dim = self.projection_expand_factor * self.model_input_dims\n",
        "        self.delta_t_rank = math.ceil(self.model_input_dims / 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhanced_model_section"
      },
      "source": [
        "## Enhanced Mamba-UNet with Feature Fusion\n",
        "\n",
        "This is the main model that integrates Feature Fusion with Depthwise Convolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enhanced_model"
      },
      "outputs": [],
      "source": [
        "class MambaUNetWithFeatureFusion(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder path with more channels\n",
        "        self.encoder1 = DoubleConvWithResidual(3, 64)  # Input: 3 RGB channels\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.fusion1 = DepthwiseFeatureFusion(64)\n",
        "\n",
        "        self.encoder2 = DoubleConvWithResidual(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fusion2 = DepthwiseFeatureFusion(128)\n",
        "\n",
        "        self.encoder3 = DoubleConvWithResidual(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.fusion3 = DepthwiseFeatureFusion(256)\n",
        "\n",
        "        # Mamba blocks in the bottleneck\n",
        "        self.mamba_blocks = nn.Sequential(*[ResidualBlock(args) for _ in range(args.num_layers)])\n",
        "\n",
        "        # Bridge between CNN and Mamba: Projection to adjust dimensions\n",
        "        self.bridge_down = nn.Conv2d(256, args.model_input_dims, kernel_size=1)\n",
        "        self.bridge_up = nn.Conv2d(args.model_input_dims, 256, kernel_size=1)\n",
        "        \n",
        "        # Feature fusion for bottleneck\n",
        "        self.bottleneck_fusion = DepthwiseFeatureFusion(256)\n",
        "\n",
        "        # Decoder path with skip connections and deep supervision\n",
        "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder3 = DoubleConvWithResidual(256, 128)  # 256 = 128 + 128 (skip)\n",
        "        self.dec_fusion3 = DepthwiseFeatureFusion(128)\n",
        "        self.deep_sup3 = nn.Conv2d(128, args.num_classes, kernel_size=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder2 = DoubleConvWithResidual(128, 64)  # 128 = 64 + 64 (skip)\n",
        "        self.dec_fusion2 = DepthwiseFeatureFusion(64)\n",
        "        self.deep_sup2 = nn.Conv2d(64, args.num_classes, kernel_size=1)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.decoder1 = DoubleConvWithResidual(32 + 3, 32)  # 32+3 = 32 (from upconv1) + 3 (original input)\n",
        "        self.dec_fusion1 = DepthwiseFeatureFusion(32)\n",
        "\n",
        "        # Multi-scale feature fusion module\n",
        "        self.multi_scale_fusion = nn.Conv2d(32 + 64 + 128, 32, kernel_size=1)\n",
        "        self.final_fusion = DepthwiseFeatureFusion(32)\n",
        "\n",
        "        # Final layer\n",
        "        self.final_conv = nn.Conv2d(32, args.num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, return_deep=False):\n",
        "        # Original input for skip connection at final layer\n",
        "        input_x = x\n",
        "        \n",
        "        # Encoder path with feature fusion\n",
        "        enc1 = self.encoder1(x)      # 64 channels\n",
        "        enc1 = self.fusion1(enc1)    # Apply feature fusion\n",
        "        enc1_pool = self.pool1(enc1)\n",
        "\n",
        "        enc2 = self.encoder2(enc1_pool)  # 128 channels\n",
        "        enc2 = self.fusion2(enc2)        # Apply feature fusion\n",
        "        enc2_pool = self.pool2(enc2)\n",
        "\n",
        "        enc3 = self.encoder3(enc2_pool)  # 256 channels\n",
        "        enc3 = self.fusion3(enc3)        # Apply feature fusion\n",
        "        enc3_pool = self.pool3(enc3)\n",
        "\n",
        "        # Bridge to Mamba\n",
        "        bridge_out = self.bridge_down(enc3_pool)\n",
        "\n",
        "        # Reshape for Mamba blocks\n",
        "        b, c, h, w = bridge_out.size()\n",
        "        mamba_input = bridge_out.permute(0, 2, 3, 1).reshape(b, h * w, c)\n",
        "\n",
        "        # Apply Mamba blocks\n",
        "        mamba_output = self.mamba_blocks(mamba_input)\n",
        "\n",
        "        # Reshape back to 2D\n",
        "        mamba_output = mamba_output.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
        "\n",
        "        # Bridge back to CNN\n",
        "        mamba_output = self.bridge_up(mamba_output)\n",
        "        \n",
        "        # Apply feature fusion to bottleneck\n",
        "        mamba_output = self.bottleneck_fusion(mamba_output, enc3_pool)\n",
        "\n",
        "        # Decoder path with skip connections and feature fusion\n",
        "        dec3 = self.upconv3(mamba_output)\n",
        "        # Ensure dimensions match\n",
        "        if dec3.shape[2:] != enc2.shape[2:]:\n",
        "            dec3 = F.interpolate(dec3, size=enc2.shape[2:], mode='bilinear', align_corners=True)\n",
        "        dec3 = torch.cat([dec3, enc2], dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec3 = self.dec_fusion3(dec3)  # Apply feature fusion\n",
        "        deep_out3 = self.deep_sup3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        if dec2.shape[2:] != enc1.shape[2:]:\n",
        "            dec2 = F.interpolate(dec2, size=enc1.shape[2:], mode='bilinear', align_corners=True)\n",
        "        dec2 = torch.cat([dec2, enc1], dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec2 = self.dec_fusion2(dec2)  # Apply feature fusion\n",
        "        deep_out2 = self.deep_sup2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        # Interpolate to match original input size and add skip connection to input\n",
        "        if dec1.shape[2:] != input_x.shape[2:]:\n",
        "            dec1 = F.interpolate(dec1, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n",
        "        dec1 = torch.cat([dec1, input_x], dim=1)  # Skip connection to original input\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        dec1 = self.dec_fusion1(dec1)  # Apply feature fusion\n",
        "        \n",
        "        # Multi-scale feature fusion\n",
        "        # Upsample dec2 and dec3 to match dec1 size\n",
        "        dec2_up = F.interpolate(dec2, size=dec1.shape[2:], mode='bilinear', align_corners=True)\n",
        "        dec3_up = F.interpolate(dec3, size=dec1.shape[2:], mode='bilinear', align_corners=True)\n",
        "        \n",
        "        # Concatenate features from different scales\n",
        "        multi_scale_features = torch.cat([dec1, dec2_up, dec3_up], dim=1)\n",
        "        fused_features = self.multi_scale_fusion(multi_scale_features)\n",
        "        fused_features = self.final_fusion(fused_features)\n",
        "\n",
        "        # Final layer\n",
        "        out = self.final_conv(fused_features)\n",
        "\n",
        "        if return_deep:\n",
        "            # Return main output and deep supervision outputs\n",
        "            deep_out2 = F.interpolate(deep_out2, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n",
        "            deep_out3 = F.interpolate(deep_out3, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n",
        "            return out, deep_out2, deep_out3\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deep_supervision_loss_section"
      },
      "source": [
        "## Deep Supervision Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deep_supervision_loss"
      },
      "outputs": [],
      "source": [
        "class DeepSupervisionLoss(nn.Module):\n",
        "    def __init__(self, main_weight=0.6, deep2_weight=0.2, deep3_weight=0.2):\n",
        "        super(DeepSupervisionLoss, self).__init__()\n",
        "        self.main_weight = main_weight\n",
        "        self.deep2_weight = deep2_weight\n",
        "        self.deep3_weight = deep3_weight\n",
        "        self.criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n",
        "\n",
        "    def forward(self, outputs, target):\n",
        "        main_out, deep2, deep3 = outputs\n",
        "\n",
        "        loss_main = self.criterion(main_out, target)\n",
        "        loss_deep2 = self.criterion(deep2, target)\n",
        "        loss_deep3 = self.criterion(deep3, target)\n",
        "\n",
        "        total_loss = (\n",
        "            self.main_weight * loss_main +\n",
        "            self.deep2_weight * loss_deep2 +\n",
        "            self.deep3_weight * loss_deep3\n",
        "        )\n",
        "\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation_metrics_section"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation_metrics"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(pred_mask, gt_mask):\n",
        "    \"\"\"Calculate IoU for binary segmentation\"\"\"\n",
        "    pred_mask = (pred_mask > 0).cpu().numpy().astype(bool)\n",
        "    gt_mask = (gt_mask > 0).cpu().numpy().astype(bool)\n",
        "\n",
        "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
        "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
        "\n",
        "    if union == 0:\n",
        "        return 1.0  # If both masks are empty, IoU is 1\n",
        "\n",
        "    return intersection / union\n",
        "\n",
        "def calculate_dice(pred_mask, gt_mask):\n",
        "    \"\"\"Calculate Dice coefficient\"\"\"\n",
        "    pred_mask = (pred_mask > 0).cpu().numpy().astype(bool)\n",
        "    gt_mask = (gt_mask > 0).cpu().numpy().astype(bool)\n",
        "\n",
        "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
        "    sum_areas = pred_mask.sum() + gt_mask.sum()\n",
        "\n",
        "    if sum_areas == 0:\n",
        "        return 1.0  # If both masks are empty, Dice is 1\n",
        "\n",
        "    return 2.0 * intersection / sum_areas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_functions_section"
      },
      "source": [
        "## Training and Validation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_functions"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch_enhanced(model, dataloader, optimizer, criterion, device, scheduler=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_iou = 0.0\n",
        "    running_dice = 0.0\n",
        "    sample_count = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "\n",
        "    for i, (images, masks) in enumerate(pbar):\n",
        "        # Move data to device\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Check data shape for the first batch\n",
        "        if i == 0:\n",
        "            print(f\"Training batch - Images: {images.shape}, Masks: {masks.shape}\")\n",
        "            print(f\"Masks unique values: {torch.unique(masks)}\")\n",
        "\n",
        "        # Forward pass with deep supervision\n",
        "        outputs = model(images, return_deep=True)\n",
        "\n",
        "        # Calculate loss with deep supervision\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Optional gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step scheduler if provided\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Get main output for metrics calculation\n",
        "        main_output = outputs[0]\n",
        "\n",
        "        # Calculate metrics\n",
        "        batch_size = images.size(0)\n",
        "        preds = torch.argmax(main_output, dim=1)\n",
        "\n",
        "        # Update statistics\n",
        "        running_loss += loss.item() * batch_size\n",
        "\n",
        "        # Calculate metrics per image\n",
        "        batch_iou = 0\n",
        "        batch_dice = 0\n",
        "        for j in range(batch_size):\n",
        "            iou = calculate_iou(preds[j], masks[j])\n",
        "            dice = calculate_dice(preds[j], masks[j])\n",
        "            batch_iou += iou\n",
        "            batch_dice += dice\n",
        "\n",
        "        running_iou += batch_iou\n",
        "        running_dice += batch_dice\n",
        "        sample_count += batch_size\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': loss.item(),\n",
        "            'iou': batch_iou / batch_size,\n",
        "            'dice': batch_dice / batch_size\n",
        "        })\n",
        "\n",
        "        # Clear some GPU memory if needed\n",
        "        del outputs, loss, preds\n",
        "        if i % 10 == 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate epoch statistics\n",
        "    epoch_loss = running_loss / sample_count\n",
        "    epoch_iou = running_iou / sample_count\n",
        "    epoch_dice = running_dice / sample_count\n",
        "\n",
        "    return epoch_loss, epoch_iou, epoch_dice\n",
        "\n",
        "def validate_enhanced(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_iou = 0.0\n",
        "    running_dice = 0.0\n",
        "    sample_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(dataloader, desc='Validation'):\n",
        "            # Move data to device\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Forward pass with deep supervision\n",
        "            outputs = model(images, return_deep=True)\n",
        "\n",
        "            # Calculate loss with deep supervision\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            # Get main output for metrics calculation\n",
        "            main_output = outputs[0]\n",
        "\n",
        "            # Calculate metrics\n",
        "            batch_size = images.size(0)\n",
        "            preds = torch.argmax(main_output, dim=1)\n",
        "\n",
        "            # Update statistics\n",
        "            running_loss += loss.item() * batch_size\n",
        "\n",
        "            # Calculate metrics per image\n",
        "            for j in range(batch_size):\n",
        "                iou = calculate_iou(preds[j], masks[j])\n",
        "                dice = calculate_dice(preds[j], masks[j])\n",
        "                running_iou += iou\n",
        "                running_dice += dice\n",
        "\n",
        "            sample_count += batch_size\n",
        "\n",
        "    # Calculate statistics\n",
        "    val_loss = running_loss / sample_count\n",
        "    val_iou = running_iou / sample_count\n",
        "    val_dice = running_dice / sample_count\n",
        "\n",
        "    return val_loss, val_iou, val_dice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_section"
      },
      "source": [
        "## Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization"
      },
      "outputs": [],
      "source": [
        "def visualize_results(model, dataloader, device, num_samples=3):\n",
        "    model.eval()\n",
        "\n",
        "    # Get a batch of data\n",
        "    images, masks = next(iter(dataloader))\n",
        "    images = images[:num_samples].to(device)\n",
        "    masks = masks[:num_samples].to(device)\n",
        "\n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Denormalize images for visualization\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
        "    images = images * std + mean\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Display original image\n",
        "        axes[i, 0].imshow(images[i].permute(1, 2, 0).cpu().numpy())\n",
        "        axes[i, 0].set_title(\"Original Image\")\n",
        "        axes[i, 0].axis(\"off\")\n",
        "\n",
        "        # Display ground truth mask\n",
        "        axes[i, 1].imshow(masks[i].cpu().numpy(), cmap=\"gray\")\n",
        "        axes[i, 1].set_title(\"Ground Truth\")\n",
        "        axes[i, 1].axis(\"off\")\n",
        "\n",
        "        # Display predicted mask\n",
        "        axes[i, 2].imshow(predictions[i].cpu().numpy(), cmap=\"gray\")\n",
        "        axes[i, 2].set_title(\"Prediction\")\n",
        "        axes[i, 2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"mamba_featurefusion_results.png\")\n",
        "    plt.show()\n",
        "    \n",
        "def plot_training_progress(history, current_epoch):\n",
        "    \"\"\"Plot training and validation metrics.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot losses\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss')\n",
        "    axes[0].plot(history['val_loss'], label='Validation Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title(f'Training and Validation Loss (Epoch {current_epoch})')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "    \n",
        "    # Plot IoU and Dice\n",
        "    axes[1].plot(history['train_iou'], label='Train IoU')\n",
        "    axes[1].plot(history['val_iou'], label='Validation IoU')\n",
        "    axes[1].plot(history['train_dice'], label='Train Dice', linestyle='--')\n",
        "    axes[1].plot(history['val_dice'], label='Validation Dice', linestyle='--')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Score')\n",
        "    axes[1].set_title(f'IoU and Dice Scores (Epoch {current_epoch})')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"training_progress_epoch_{current_epoch}.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main_section"
      },
      "source": [
        "## Main Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count":