{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6047955,"sourceType":"datasetVersion","datasetId":3459850}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n!pip install gdown tqdm matplotlib pillow einops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T18:55:48.151811Z","iopub.execute_input":"2025-05-28T18:55:48.152454Z","iopub.status.idle":"2025-05-28T18:55:52.338575Z","shell.execute_reply.started":"2025-05-28T18:55:48.152429Z","shell.execute_reply":"2025-05-28T18:55:52.337751Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->matplotlib) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->matplotlib) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom einops import rearrange, repeat\nimport math\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport requests\nimport zipfile\nimport shutil\nimport glob\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nimport kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T18:55:52.340315Z","iopub.execute_input":"2025-05-28T18:55:52.340846Z","iopub.status.idle":"2025-05-28T18:55:57.825880Z","shell.execute_reply.started":"2025-05-28T18:55:52.340808Z","shell.execute_reply":"2025-05-28T18:55:57.825351Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# -------------------------\n# Multi-Kernel Positional Embedding Module\n# -------------------------\nclass MultiKernelPositionalEmbedding(nn.Module):\n    def __init__(self, in_channels, reduction=8):\n        super(MultiKernelPositionalEmbedding, self).__init__()\n        self.mid_channels = max(8, in_channels // reduction)\n        \n        # Multiple kernels of different sizes to capture multi-scale spatial information\n        self.conv3x3 = nn.Conv2d(in_channels, self.mid_channels, kernel_size=3, padding=1)\n        self.conv5x5 = nn.Conv2d(in_channels, self.mid_channels, kernel_size=5, padding=2)\n        self.conv7x7 = nn.Conv2d(in_channels, self.mid_channels, kernel_size=7, padding=3)\n        \n        # Position-sensitive attention\n        self.position_attention = nn.Sequential(\n            nn.Conv2d(self.mid_channels * 3, in_channels, kernel_size=1),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, in_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        # Extract multi-scale features\n        feat_3x3 = self.conv3x3(x)\n        feat_5x5 = self.conv5x5(x)\n        feat_7x7 = self.conv7x7(x)\n        \n        # Concatenate multi-scale features\n        multi_scale_feat = torch.cat([feat_3x3, feat_5x5, feat_7x7], dim=1)\n        \n        # Generate position-sensitive attention map\n        attention_map = self.position_attention(multi_scale_feat)\n        \n        # Apply attention to input features\n        enhanced = x * attention_map\n        \n        return enhanced\n\n# -------------------------\n# Double Convolution with MKPE\n# -------------------------\nclass DoubleConvWithMKPE(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.same_channels = in_channels == out_channels\n        \n        # Double convolution\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Multi-Kernel Positional Embedding\n        self.mkpe = MultiKernelPositionalEmbedding(out_channels)\n        \n        # Optional projection for residual connection\n        if not self.same_channels:\n            self.project = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n            \n    def forward(self, x):\n        identity = x if self.same_channels else self.project(x)\n        \n        # Double convolution\n        x = self.double_conv(x)\n        \n        # Apply Multi-Kernel Positional Embedding\n        x = self.mkpe(x)\n        \n        # Residual connection\n        x = x + identity\n        \n        return x\n\n# -------------------------\n# Dataset Download and Setup - Adapted for Kaggle\n# -------------------------\n# def download_and_setup_dataset(force_download=False):\n#     \"\"\"Download and properly set up Kvasir-SEG dataset\"\"\"\n#     base_path = '/kaggle/working/datasets'\n#     kvasir_path = os.path.join(base_path, 'kvasir-seg')\n    \n#     # First check if dataset exists in Kaggle input directory\n#     kaggle_input_path = '/kaggle/input'\n#     for dirname, _, _ in os.walk(kaggle_input_path):\n#         if 'kvasir-seg' in dirname.lower() and os.path.exists(os.path.join(dirname, 'images')):\n#             print(f\"Found Kvasir-SEG dataset at {dirname}\")\n#             return dirname\n\n#     # Make sure base directory exists\n#     os.makedirs(base_path, exist_ok=True)\n\n#     # Check if dataset already exists in the expected directory structure\n#     if os.path.exists(os.path.join(kvasir_path, 'images')) and \\\n#        os.path.exists(os.path.join(kvasir_path, 'masks')) and \\\n#        len(os.listdir(os.path.join(kvasir_path, 'images'))) > 0 and \\\n#        not force_download:\n#         print(\"Kvasir-SEG dataset already exists.\")\n#         return kvasir_path\n\n#     # Direct URL to the zip file\n#     dataset_url = \"https://datasets.simula.no/downloads/kvasir-seg.zip\"\n#     zip_path = os.path.join(base_path, 'kvasir-seg.zip')\n\n#     # Download the dataset\n#     print(\"Downloading Kvasir-SEG dataset...\")\n#     try:\n#         response = requests.get(dataset_url, stream=True)\n#         total_size = int(response.headers.get('content-length', 0))\n#         block_size = 1024\n#         progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n\n#         with open(zip_path, 'wb') as f:\n#             for data in response.iter_content(block_size):\n#                 progress_bar.update(len(data))\n#                 f.write(data)\n#         progress_bar.close()\n#     except Exception as e:\n#         print(f\"Error downloading dataset: {e}\")\n#         return None\n\n#     print(f\"Download completed, file saved to {zip_path}\")\n\n#     # Extract the dataset\n#     print(\"Extracting dataset...\")\n#     try:\n#         with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n#             zip_ref.extractall(base_path)\n#         print(\"Extraction completed.\")\n#     except Exception as e:\n#         print(f\"Error extracting dataset: {e}\")\n#         return None\n\n#     # Check the structure of extracted files\n#     extracted_files = glob.glob(os.path.join(base_path, \"**\"), recursive=True)\n#     print(\"Extracted file structure:\")\n#     for file in extracted_files[:10]:  # Show only first 10 files\n#         print(f\"  {file}\")\n#     if len(extracted_files) > 10:\n#         print(f\"  ... and {len(extracted_files)-10} more files\")\n\n#     # Locate the images and masks directories\n#     image_dirs = glob.glob(os.path.join(base_path, \"**/images\"), recursive=True)\n#     mask_dirs = glob.glob(os.path.join(base_path, \"**/masks\"), recursive=True)\n\n#     print(f\"Found image directories: {image_dirs}\")\n#     print(f\"Found mask directories: {mask_dirs}\")\n\n#     # Ensure proper directory structure\n#     os.makedirs(os.path.join(kvasir_path, 'images'), exist_ok=True)\n#     os.makedirs(os.path.join(kvasir_path, 'masks'), exist_ok=True)\n\n#     # Copy files to the expected location if needed\n#     if image_dirs and mask_dirs:\n#         src_image_dir = image_dirs[0]\n#         src_mask_dir = mask_dirs[0]\n\n#         if src_image_dir != os.path.join(kvasir_path, 'images'):\n#             print(f\"Moving images from {src_image_dir} to {os.path.join(kvasir_path, 'images')}\")\n#             for img_file in os.listdir(src_image_dir):\n#                 shutil.copy(\n#                     os.path.join(src_image_dir, img_file),\n#                     os.path.join(kvasir_path, 'images', img_file)\n#                 )\n\n#         if src_mask_dir != os.path.join(kvasir_path, 'masks'):\n#             print(f\"Moving masks from {src_mask_dir} to {os.path.join(kvasir_path, 'masks')}\")\n#             for mask_file in os.listdir(src_mask_dir):\n#                 shutil.copy(\n#                     os.path.join(src_mask_dir, mask_file),\n#                     os.path.join(kvasir_path, 'masks', mask_file)\n#                 )\n\n#     # Clean up\n#     try:\n#         os.remove(zip_path)\n#         print(\"Removed zip file.\")\n#     except:\n#         print(\"Could not remove zip file.\")\n\n#     # Verify the dataset is now properly set up\n#     if os.path.exists(os.path.join(kvasir_path, 'images')) and \\\n#        os.path.exists(os.path.join(kvasir_path, 'masks')) and \\\n#        len(os.listdir(os.path.join(kvasir_path, 'images'))) > 0:\n#         print(\"Dataset setup completed successfully.\")\n#         print(f\"Found {len(os.listdir(os.path.join(kvasir_path, 'images')))} images and \"\n#               f\"{len(os.listdir(os.path.join(kvasir_path, 'masks')))} masks.\")\n#         return kvasir_path\n#     else:\n#         print(\"Dataset setup failed.\")\n#         return None\n\n# # -------------------------\n# # Dataset class - No changes\n# # -------------------------\n# class KvasirSEGDataset(Dataset):\n#     def __init__(self, root_dir, split='train', transform=None, target_transform=None, augment=True):\n#         self.root_dir = root_dir\n#         self.transform = transform\n#         self.target_transform = target_transform\n#         self.augment = augment and split == 'train'  # Only augment training data\n#         self.img_dir = os.path.join(root_dir, 'images')\n#         self.mask_dir = os.path.join(root_dir, 'masks')\n\n#         # Verify directories exist\n#         if not os.path.exists(self.img_dir):\n#             raise ValueError(f\"Images directory not found: {self.img_dir}\")\n#         if not os.path.exists(self.mask_dir):\n#             raise ValueError(f\"Masks directory not found: {self.mask_dir}\")\n\n#         # Get all image files\n#         self.images = sorted([f for f in os.listdir(self.img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n#         if not self.images:\n#             raise ValueError(f\"No images found in {self.img_dir}\")\n\n#         # Print some sample image names for debugging\n#         print(f\"Sample image names: {self.images[:5]}\")\n\n#         # Split data into train/val/test (80/10/10 split)\n#         np.random.seed(42)  # For reproducibility\n#         indices = np.random.permutation(len(self.images))\n\n#         if split == 'train':\n#             self.images = [self.images[i] for i in indices[:int(0.8 * len(self.images))]]\n#         elif split == 'val':\n#             self.images = [self.images[i] for i in indices[int(0.8 * len(self.images)):int(0.9 * len(self.images))]]\n#         else:  # test\n#             self.images = [self.images[i] for i in indices[int(0.9 * len(self.images)):]]\n\n#         print(f\"Created {split} dataset with {len(self.images)} images\")\n\n#     def __len__(self):\n#         return len(self.images)\n\n#     def __getitem__(self, idx):\n#         img_name = self.images[idx]\n#         img_path = os.path.join(self.img_dir, img_name)\n\n#         # Find corresponding mask\n#         base_name = os.path.splitext(img_name)[0]\n#         mask_candidates = [\n#             os.path.join(self.mask_dir, base_name + ext)\n#             for ext in ['.jpg', '.png', '.jpeg', '.tif']\n#         ]\n#         mask_path = next((path for path in mask_candidates if os.path.exists(path)), None)\n\n#         if not mask_path:\n#             # Look for files that start with the same name\n#             mask_files = os.listdir(self.mask_dir)\n#             matches = [f for f in mask_files if f.startswith(base_name)]\n#             if matches:\n#                 mask_path = os.path.join(self.mask_dir, matches[0])\n#             else:\n#                 raise FileNotFoundError(f\"No mask found for image {img_name}\")\n\n#         # Print paths for debugging (only for the first item)\n#         if idx == 0:\n#             print(f\"Image path: {img_path}\")\n#             print(f\"Mask path: {mask_path}\")\n\n#         # Load image and mask\n#         image = Image.open(img_path).convert(\"RGB\")\n#         mask = Image.open(mask_path).convert(\"L\")\n\n#         # Apply augmentation if enabled\n#         if self.augment:\n#             # Random horizontal flip\n#             if random.random() > 0.5:\n#                 image = TF.hflip(image)\n#                 mask = TF.hflip(mask)\n\n#             # Random vertical flip\n#             if random.random() > 0.5:\n#                 image = TF.vflip(image)\n#                 mask = TF.vflip(mask)\n\n#             # Random rotation\n#             if random.random() > 0.5:\n#                 angle = random.choice([90, 180, 270])\n#                 fill = 0\n#                 image = TF.rotate(image, angle, fill=fill)\n#                 mask = TF.rotate(mask, angle, fill=fill)\n\n#             # Color jitter (only for image)\n#             if random.random() > 0.5:\n#                 image = TF.adjust_brightness(image, random.uniform(0.8, 1.2))\n#                 image = TF.adjust_contrast(image, random.uniform(0.8, 1.2))\n#                 image = TF.adjust_saturation(image, random.uniform(0.8, 1.2))\n\n#         # Apply transformations\n#         if self.transform:\n#             image = self.transform(image)\n\n#         if self.target_transform:\n#             mask = self.target_transform(mask)\n#         else:\n#             # Default transformation for masks\n#             mask_array = np.array(mask)\n#             mask_binary = (mask_array > 0).astype(np.int64)\n#             mask = torch.from_numpy(mask_binary).long()  # Explicit cast to long\n\n#         # For debugging: print data types and ranges (only for the first item)\n#         if idx == 0:\n#             print(f\"Image shape: {image.shape}, dtype: {image.dtype}, range: [{image.min()}, {image.max()}]\")\n#             print(f\"Mask shape: {mask.shape}, dtype: {mask.dtype}, range: [{mask.min()}, {mask.max()}]\")\n\n#         # Ensure mask is 2D (H,W) not 3D (1,H,W)\n#         if mask.dim() == 3 and mask.size(0) == 1:\n#             mask = mask.squeeze(0)\n\n#         return image, mask\n\n# -------------------------\n# Dataset Download and Setup - Using kagglehub\n# -------------------------\ndef download_and_setup_dataset(force_download=False):\n    \"\"\"Download and properly set up only the 'positive' data from PolypGen dataset\"\"\"\n    base_path = '/kaggle/working/datasets'\n    polypgen_path = os.path.join(base_path, 'polypgen')\n\n    # Check if dataset exists in Kaggle input directory\n    kaggle_input_path = '/kaggle/input'\n    for dirname, _, _ in os.walk(kaggle_input_path):\n        if 'polypgen' in dirname.lower():\n            positive_path = os.path.join(dirname, 'positive')\n            if os.path.exists(os.path.join(positive_path, 'images')) and \\\n               os.path.exists(os.path.join(positive_path, 'masks')):\n                print(f\"Found PolypGen 'positive' dataset at {positive_path}\")\n                return positive_path\n\n    # If not found in input, try downloading (if kagglehub is allowed)\n    os.makedirs(base_path, exist_ok=True)\n\n    if os.path.exists(os.path.join(polypgen_path, 'images')) and \\\n       os.path.exists(os.path.join(polypgen_path, 'masks')) and \\\n       len(os.listdir(os.path.join(polypgen_path, 'images'))) > 0 and \\\n       not force_download:\n        print(\"PolypGen dataset already exists.\")\n        return polypgen_path\n\n    print(\"Downloading PolypGen dataset using kagglehub...\")\n    try:\n        downloaded_path = kagglehub.dataset_download(\"kokoroou/polypgen2021\")\n    except Exception as e:\n        print(f\"Error downloading dataset: {e}\")\n        return None\n\n    print(f\"Dataset downloaded to {downloaded_path}\")\n\n    # Find only the positive/image and positive/mask folders\n    image_dirs = glob.glob(os.path.join(downloaded_path, \"**/positive/images\"), recursive=True)\n    mask_dirs = glob.glob(os.path.join(downloaded_path, \"**/positive/masks\"), recursive=True)\n\n    print(f\"Found positive image directories: {image_dirs}\")\n    print(f\"Found positive mask directories: {mask_dirs}\")\n\n    os.makedirs(os.path.join(polypgen_path, 'images'), exist_ok=True)\n    os.makedirs(os.path.join(polypgen_path, 'masks'), exist_ok=True)\n\n    if image_dirs and mask_dirs:\n        src_image_dir = image_dirs[0]\n        src_mask_dir = mask_dirs[0]\n\n        print(f\"Copying images from {src_image_dir} to {os.path.join(polypgen_path, 'images')}\")\n        for img_file in os.listdir(src_image_dir):\n            shutil.copy(\n                os.path.join(src_image_dir, img_file),\n                os.path.join(polypgen_path, 'images', img_file)\n            )\n\n        print(f\"Copying masks from {src_mask_dir} to {os.path.join(polypgen_path, 'masks')}\")\n        for mask_file in os.listdir(src_mask_dir):\n            shutil.copy(\n                os.path.join(src_mask_dir, mask_file),\n                os.path.join(polypgen_path, 'masks', mask_file)\n            )\n\n    if os.path.exists(os.path.join(polypgen_path, 'images')) and \\\n       os.path.exists(os.path.join(polypgen_path, 'masks')) and \\\n       len(os.listdir(os.path.join(polypgen_path, 'images'))) > 0:\n        print(\"Dataset setup completed successfully.\")\n        print(f\"Found {len(os.listdir(os.path.join(polypgen_path, 'images')))} images and \"\n              f\"{len(os.listdir(os.path.join(polypgen_path, 'masks')))} masks.\")\n        return polypgen_path\n    else:\n        print(\"Dataset setup failed.\")\n        return None\n\n# -------------------------\n# Dataset class - No changes\n# -------------------------\nclass PolypGenDataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None, target_transform=None, augment=True):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.target_transform = target_transform\n        self.augment = augment and split == 'train'  # Only augment training data\n        self.img_dir = os.path.join(root_dir, 'images')\n        self.mask_dir = os.path.join(root_dir, 'masks')\n\n        # Verify directories exist\n        if not os.path.exists(self.img_dir):\n            raise ValueError(f\"Images directory not found: {self.img_dir}\")\n        if not os.path.exists(self.mask_dir):\n            raise ValueError(f\"Masks directory not found: {self.mask_dir}\")\n\n        # Get all image files\n        self.images = sorted([f for f in os.listdir(self.img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n        if not self.images:\n            raise ValueError(f\"No images found in {self.img_dir}\")\n\n        # Print some sample image names for debugging\n        print(f\"Sample image names: {self.images[:5]}\")\n\n        # Split data into train/val/test (80/10/10 split)\n        np.random.seed(42)  # For reproducibility\n        indices = np.random.permutation(len(self.images))\n\n        if split == 'train':\n            self.images = [self.images[i] for i in indices[:int(0.8 * len(self.images))]]\n        elif split == 'val':\n            self.images = [self.images[i] for i in indices[int(0.8 * len(self.images)):int(0.9 * len(self.images))]]\n        else:  # test\n            self.images = [self.images[i] for i in indices[int(0.9 * len(self.images)):]]\n\n        print(f\"Created {split} dataset with {len(self.images)} images\")\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n\n        # Find corresponding mask\n        base_name = os.path.splitext(img_name)[0]\n        mask_candidates = [\n            os.path.join(self.mask_dir, base_name + ext)\n            for ext in ['.jpg', '.png', '.jpeg', '.tif']\n        ]\n        mask_path = next((path for path in mask_candidates if os.path.exists(path)), None)\n\n        if not mask_path:\n            # Look for files that start with the same name\n            mask_files = os.listdir(self.mask_dir)\n            matches = [f for f in mask_files if f.startswith(base_name)]\n            if matches:\n                mask_path = os.path.join(self.mask_dir, matches[0])\n            else:\n                raise FileNotFoundError(f\"No mask found for image {img_name}\")\n\n        # Print paths for debugging (only for the first item)\n        if idx == 0:\n            print(f\"Image path: {img_path}\")\n            print(f\"Mask path: {mask_path}\")\n\n        # Load image and mask\n        image = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n\n        # Apply augmentation if enabled\n        if self.augment:\n            # Random horizontal flip\n            if random.random() > 0.5:\n                image = TF.hflip(image)\n                mask = TF.hflip(mask)\n\n            # Random vertical flip\n            if random.random() > 0.5:\n                image = TF.vflip(image)\n                mask = TF.vflip(mask)\n\n            # Random rotation\n            if random.random() > 0.5:\n                angle = random.choice([90, 180, 270])\n                fill = 0\n                image = TF.rotate(image, angle, fill=fill)\n                mask = TF.rotate(mask, angle, fill=fill)\n\n            # Color jitter (only for image)\n            if random.random() > 0.5:\n                image = TF.adjust_brightness(image, random.uniform(0.8, 1.2))\n                image = TF.adjust_contrast(image, random.uniform(0.8, 1.2))\n                image = TF.adjust_saturation(image, random.uniform(0.8, 1.2))\n\n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n\n        if self.target_transform:\n            mask = self.target_transform(mask)\n        else:\n            # Default transformation for masks\n            mask_array = np.array(mask)\n            mask_binary = (mask_array > 0).astype(np.int64)\n            mask = torch.from_numpy(mask_binary).long()  # Explicit cast to long\n\n        # Ensure mask is 2D (H,W) not 3D (1,H,W)\n        if mask.dim() == 3 and mask.size(0) == 1:\n            mask = mask.squeeze(0)\n\n        return image, mask\n\n# -------------------------\n# Enhanced Selective Scan - No changes\n# -------------------------\ndef selective_scan(u, delta, A, B, C, D):\n    # Add numerical stability measures\n    A = torch.clamp(A, min=-5.0, max=5.0)\n\n    dA = torch.einsum('bld,dn->bldn', delta, A)\n    dB_u = torch.einsum('bld,bld,bln->bldn', delta, u, B)\n\n    dA_cumsum = torch.cat([dA[:, 1:], torch.zeros_like(dA[:, :1])], dim=1)\n    dA_cumsum = torch.flip(dA_cumsum, dims=[1])\n    dA_cumsum = torch.cumsum(dA_cumsum, dim=1)\n    dA_cumsum = torch.clamp(dA_cumsum, max=15.0)\n    dA_cumsum = torch.exp(dA_cumsum)\n    dA_cumsum = torch.flip(dA_cumsum, dims=[1])\n\n    x = dB_u * dA_cumsum\n    x = torch.cumsum(x, dim=1) / (dA_cumsum + 1e-6)\n\n    y = torch.einsum('bldn,bln->bld', x, C)\n    return y + u * D\n\n# -------------------------\n# Combined Loss Function - No changes\n# -------------------------\nclass CombinedLoss(nn.Module):\n    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n        super(CombinedLoss, self).__init__()\n        self.bce_weight = bce_weight\n        self.dice_weight = dice_weight\n        self.bce_loss = nn.CrossEntropyLoss()\n\n    def forward(self, inputs, targets):\n        # BCE Loss\n        bce = self.bce_loss(inputs, targets)\n\n        # Dice Loss\n        inputs_soft = F.softmax(inputs, dim=1)\n        targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n\n        # Calculate Dice loss manually\n        intersection = (inputs_soft * targets_one_hot).sum(dim=(2, 3))\n        cardinality = inputs_soft.sum(dim=(2, 3)) + targets_one_hot.sum(dim=(2, 3))\n\n        dice = (2. * intersection / (cardinality + 1e-6)).mean()\n        dice_loss = 1 - dice\n\n        # Combined loss\n        return self.bce_weight * bce + self.dice_weight * dice_loss\n\n# -------------------------\n# Improved MambaBlock - No changes\n# -------------------------\nclass MambaBlock(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.in_proj = nn.Linear(args.model_input_dims, args.model_internal_dim * 2, bias=False)\n        self.conv1d = nn.Conv1d(args.model_internal_dim, args.model_internal_dim, kernel_size=args.conv_kernel_size,\n                               padding=args.conv_kernel_size-1, groups=args.model_internal_dim)\n        self.x_proj = nn.Linear(args.model_internal_dim, args.delta_t_rank + args.model_states * 2, bias=False)\n        self.delta_proj = nn.Linear(args.delta_t_rank, args.model_internal_dim)\n\n        # Initialize A values\n        A_vals = torch.arange(1, args.model_states + 1).float() / args.model_states * 3\n        self.A_log = nn.Parameter(torch.log(repeat(A_vals, 'n -> d n', d=args.model_internal_dim)))\n        self.D = nn.Parameter(torch.ones(args.model_internal_dim))\n        self.out_proj = nn.Linear(args.model_internal_dim, args.model_input_dims, bias=args.dense_use_bias)\n\n    def forward(self, x):\n        # Use gradient checkpointing for better memory efficiency during training\n        if self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward, x, use_reentrant=False)\n        else:\n            return self._forward(x)\n\n    def _forward(self, x):\n        b, l, d = x.shape\n        x_and_res = self.in_proj(x)\n        x1, res = x_and_res.chunk(2, dim=-1)\n\n        x1 = rearrange(x1, 'b l d -> b d l')\n        x1 = self.conv1d(x1)[..., :l]\n        x1 = rearrange(x1, 'b d l -> b l d')\n        x1 = F.silu(x1)\n\n        # Apply bounded values for more stability\n        A = -torch.exp(torch.clamp(self.A_log, min=-5, max=5))\n        D = self.D\n        x_dbl = self.x_proj(x1)\n        delta, B, C = torch.split(x_dbl, [self.args.delta_t_rank, self.args.model_states, self.args.model_states], dim=-1)\n        delta = F.softplus(self.delta_proj(delta))\n\n        y = selective_scan(x1, delta, A, B, C, D)\n        y = y * F.silu(res)\n        return self.out_proj(y)\n\n# -------------------------\n# Enhanced Residual Block - No changes\n# -------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(args.model_input_dims)\n        self.mixer = MambaBlock(args)\n        self.dropout = nn.Dropout(args.dropout_rate)\n        self.norm2 = nn.LayerNorm(args.model_input_dims)\n\n    def forward(self, x):\n        residual = x\n        x = self.norm1(x)\n        x = self.mixer(x)\n        x = self.dropout(x)\n        x = residual + x\n        return self.norm2(x)\n\n# -------------------------\n# Improved Model Args - No changes\n# -------------------------\nclass ModelArgs:\n    def __init__(self):\n        # Model dimensions\n        self.model_input_dims = 96\n        self.model_states = 96\n        self.projection_expand_factor = 2\n        self.conv_kernel_size = 4\n        self.conv_use_bias = False\n        self.dense_use_bias = False\n        self.layer_id = -1\n        self.seq_length = 256\n        self.num_layers = 4\n        self.dropout_rate = 0.2\n        self.use_lm_head = False\n        self.num_classes = 2  # Binary segmentation\n        self.final_activation = 'none'\n        self.model_internal_dim = self.projection_expand_factor * self.model_input_dims\n        self.delta_t_rank = math.ceil(self.model_input_dims / 16)\n\n# -------------------------\n# Mamba-UNet with Multi-Kernel Positional Embedding\n# -------------------------\nclass MambaUNetWithMKPE(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n\n        # Encoder path with MKPE\n        self.encoder1 = DoubleConvWithMKPE(3, 64)  # Input: 3 RGB channels\n        self.pool1 = nn.MaxPool2d(2)\n\n        self.encoder2 = DoubleConvWithMKPE(64, 128)\n        self.pool2 = nn.MaxPool2d(2)\n\n        self.encoder3 = DoubleConvWithMKPE(128, 256)\n        self.pool3 = nn.MaxPool2d(2)\n\n        # Mamba blocks in the bottleneck\n        self.mamba_blocks = nn.Sequential(*[ResidualBlock(args) for _ in range(args.num_layers)])\n\n        # Bridge between CNN and Mamba\n        self.bridge_down = nn.Conv2d(256, args.model_input_dims, kernel_size=1)\n        self.bridge_up = nn.Conv2d(args.model_input_dims, 256, kernel_size=1)\n        \n        # MKPE for bottleneck features\n        self.bottleneck_mkpe = MultiKernelPositionalEmbedding(256)\n\n        # Decoder path with skip connections\n        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.decoder3 = DoubleConvWithMKPE(256, 128)  # 256 = 128 (upconv) + 128 (skip)\n        self.deep_sup3 = nn.Conv2d(128, args.num_classes, kernel_size=1)\n        self.mkpe3 = MultiKernelPositionalEmbedding(128)\n\n        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.decoder2 = DoubleConvWithMKPE(128, 64)  # 128 = 64 (upconv) + 64 (skip)\n        self.deep_sup2 = nn.Conv2d(64, args.num_classes, kernel_size=1)\n        self.mkpe2 = MultiKernelPositionalEmbedding(64)\n\n        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n        self.decoder1 = DoubleConvWithMKPE(35, 32)  # 35 = 32 (upconv) + 3 (original input)\n        self.mkpe1 = MultiKernelPositionalEmbedding(32)\n\n        # Final layer\n        self.final_conv = nn.Conv2d(32, args.num_classes, kernel_size=1)\n        \n        # Output MKPE module\n        self.output_mkpe = MultiKernelPositionalEmbedding(args.num_classes, reduction=2)\n\n    def forward(self, x, return_deep=False):\n        # Save input for skip connection\n        input_x = x\n        \n        # Encoder path\n        enc1 = self.encoder1(x)      # 64 channels\n        enc1_pool = self.pool1(enc1)\n\n        enc2 = self.encoder2(enc1_pool)  # 128 channels\n        enc2_pool = self.pool2(enc2)\n\n        enc3 = self.encoder3(enc2_pool)  # 256 channels\n        enc3_pool = self.pool3(enc3)\n\n        # Bridge to Mamba\n        bridge_out = self.bridge_down(enc3_pool)\n\n        # Reshape for Mamba blocks\n        b, c, h, w = bridge_out.size()\n        mamba_input = bridge_out.permute(0, 2, 3, 1).reshape(b, h * w, c)\n\n        # Apply Mamba blocks\n        mamba_output = self.mamba_blocks(mamba_input)\n\n        # Reshape back to 2D\n        mamba_output = mamba_output.reshape(b, h, w, c).permute(0, 3, 1, 2)\n\n        # Bridge back to CNN\n        mamba_output = self.bridge_up(mamba_output)\n        \n        # Apply MKPE to bottleneck features\n        mamba_output = self.bottleneck_mkpe(mamba_output)\n\n        # Decoder path with skip connections\n        dec3 = self.upconv3(mamba_output)\n        if dec3.shape[2:] != enc2.shape[2:]:\n            dec3 = F.interpolate(dec3, size=enc2.shape[2:], mode='bilinear', align_corners=True)\n        \n        # Concatenate and process\n        dec3 = torch.cat([dec3, enc2], dim=1)\n        dec3 = self.decoder3(dec3)\n        dec3 = self.mkpe3(dec3)  # Apply MKPE\n        deep_out3 = self.deep_sup3(dec3)\n\n        dec2 = self.upconv2(dec3)\n        if dec2.shape[2:] != enc1.shape[2:]:\n            dec2 = F.interpolate(dec2, size=enc1.shape[2:], mode='bilinear', align_corners=True)\n        \n        # Concatenate and process\n        dec2 = torch.cat([dec2, enc1], dim=1)\n        dec2 = self.decoder2(dec2)\n        dec2 = self.mkpe2(dec2)  # Apply MKPE\n        deep_out2 = self.deep_sup2(dec2)\n\n        dec1 = self.upconv1(dec2)\n        if dec1.shape[2:] != input_x.shape[2:]:\n            dec1 = F.interpolate(dec1, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n        \n        # Concatenate and process\n        dec1 = torch.cat([dec1, input_x], dim=1)  # Skip connection to original input\n        dec1 = self.decoder1(dec1)\n        dec1 = self.mkpe1(dec1)  # Apply MKPE\n\n        # Final layer with MKPE\n        out = self.final_conv(dec1)\n        out = self.output_mkpe(out)  # Output MKPE\n\n        if return_deep:\n            # Return main output and deep supervision outputs\n            deep_out2 = F.interpolate(deep_out2, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n            deep_out3 = F.interpolate(deep_out3, size=input_x.shape[2:], mode='bilinear', align_corners=True)\n            return out, deep_out2, deep_out3\n\n        return out\n\n# -------------------------\n# Deep Supervision Loss - No changes\n# -------------------------\nclass DeepSupervisionLoss(nn.Module):\n    def __init__(self, main_weight=0.6, deep2_weight=0.2, deep3_weight=0.2):\n        super(DeepSupervisionLoss, self).__init__()\n        self.main_weight = main_weight\n        self.deep2_weight = deep2_weight\n        self.deep3_weight = deep3_weight\n        self.criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n\n    def forward(self, outputs, target):\n        main_out, deep2, deep3 = outputs\n\n        loss_main = self.criterion(main_out, target)\n        loss_deep2 = self.criterion(deep2, target)\n        loss_deep3 = self.criterion(deep3, target)\n\n        total_loss = (\n            self.main_weight * loss_main +\n            self.deep2_weight * loss_deep2 +\n            self.deep3_weight * loss_deep3\n        )\n\n        return total_loss\n\n# -------------------------\n# Evaluation Metrics - No changes\n# -------------------------\ndef calculate_iou(pred_mask, gt_mask):\n    \"\"\"Calculate IoU for binary segmentation\"\"\"\n    pred_mask = (pred_mask > 0).cpu().numpy().astype(bool)\n    gt_mask = (gt_mask > 0).cpu().numpy().astype(bool)\n\n    intersection = np.logical_and(pred_mask, gt_mask).sum()\n    union = np.logical_or(pred_mask, gt_mask).sum()\n\n    if union == 0:\n        return 1.0  # If both masks are empty, IoU is 1\n\n    return intersection / union\n\ndef calculate_dice(pred_mask, gt_mask):\n    \"\"\"Calculate Dice coefficient\"\"\"\n    pred_mask = (pred_mask > 0).cpu().numpy().astype(bool)\n    gt_mask = (gt_mask > 0).cpu().numpy().astype(bool)\n\n    intersection = np.logical_and(pred_mask, gt_mask).sum()\n    sum_areas = pred_mask.sum() + gt_mask.sum()\n\n    if sum_areas == 0:\n        return 1.0  # If both masks are empty, Dice is 1\n\n    return 2.0 * intersection / sum_areas\n\n# -------------------------\n# Plot training progress - Adapted for Kaggle\n# -------------------------\ndef plot_training_progress(history, epoch):\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(history['train_iou'], label='Train IoU')\n    plt.plot(history['val_iou'], label='Val IoU')\n    plt.title('IoU')\n    plt.xlabel('Epoch')\n    plt.legend()\n    \n    plt.subplot(1, 3, 3)\n    plt.plot(history['train_dice'], label='Train Dice')\n    plt.plot(history['val_dice'], label='Val Dice')\n    plt.title('Dice')\n    plt.xlabel('Epoch')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(f\"/kaggle/working/training_progress_epoch_{epoch}.png\")\n    plt.close()\n\n# -------------------------\n# Enhanced Training Function - No changes\n# -------------------------\ndef train_one_epoch_enhanced(model, dataloader, optimizer, criterion, device, scheduler=None):\n    model.train()\n    running_loss = 0.0\n    running_iou = 0.0\n    running_dice = 0.0\n    sample_count = 0\n\n    pbar = tqdm(dataloader, desc='Training')\n\n    for i, (images, masks) in enumerate(pbar):\n        # Move data to device\n        images = images.to(device)\n        masks = masks.to(device)\n\n        # Check data shape for the first batch\n        if i == 0:\n            print(f\"Training batch - Images: {images.shape}, Masks: {masks.shape}\")\n            print(f\"Masks unique values: {torch.unique(masks)}\")\n\n        # Forward pass with deep supervision\n        outputs = model(images, return_deep=True)\n\n        # Calculate loss with deep supervision\n        loss = criterion(outputs, masks)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        # Optional gradient clipping for stability\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        # Step scheduler if provided\n        if scheduler is not None:\n            scheduler.step()\n\n        # Get main output for metrics calculation\n        main_output = outputs[0]\n\n        # Calculate metrics\n        batch_size = images.size(0)\n        preds = torch.argmax(main_output, dim=1)\n\n        # Update statistics\n        running_loss += loss.item() * batch_size\n\n        # Calculate metrics per image\n        batch_iou = 0\n        batch_dice = 0\n        for j in range(batch_size):\n            iou = calculate_iou(preds[j], masks[j])\n            dice = calculate_dice(preds[j], masks[j])\n            batch_iou += iou\n            batch_dice += dice\n\n        running_iou += batch_iou\n        running_dice += batch_dice\n        sample_count += batch_size\n\n        # Update progress bar\n        pbar.set_postfix({\n            'loss': loss.item(),\n            'iou': batch_iou / batch_size,\n            'dice': batch_dice / batch_size\n        })\n\n        # Clear some GPU memory if needed\n        del outputs, loss, preds\n        if i % 10 == 0 and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    # Calculate epoch statistics\n    epoch_loss = running_loss / sample_count\n    epoch_iou = running_iou / sample_count\n    epoch_dice = running_dice / sample_count\n\n    return epoch_loss, epoch_iou, epoch_dice\n\n# -------------------------\n# Validation Function - No changes\n# -------------------------\ndef validate_enhanced(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_iou = 0.0\n    running_dice = 0.0\n    sample_count = 0\n\n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc='Validation'):\n            # Move data to device\n            images = images.to(device)\n            masks = masks.to(device)\n\n            # Forward pass with deep supervision\n            outputs = model(images, return_deep=True)\n\n            # Calculate loss with deep supervision\n            loss = criterion(outputs, masks)\n\n            # Get main output for metrics calculation\n            main_output = outputs[0]\n\n            # Calculate metrics\n            batch_size = images.size(0)\n            preds = torch.argmax(main_output, dim=1)\n\n            # Update statistics\n            running_loss += loss.item() * batch_size\n\n            # Calculate metrics per image\n            for j in range(batch_size):\n                iou = calculate_iou(preds[j], masks[j])\n                dice = calculate_dice(preds[j], masks[j])\n                running_iou += iou\n                running_dice += dice\n\n            sample_count += batch_size\n\n    # Calculate statistics\n    val_loss = running_loss / sample_count\n    val_iou = running_iou / sample_count\n    val_dice = running_dice / sample_count\n\n    return val_loss, val_iou, val_dice\n\n# -------------------------\n# Visualization Function - Adapted for Kaggle\n# -------------------------\ndef visualize_results(model, dataloader, device, num_samples=3):\n    model.eval()\n\n    # Get a batch of data\n    images, masks = next(iter(dataloader))\n    images = images[:num_samples].to(device)\n    masks = masks[:num_samples].to(device)\n\n    # Generate predictions\n    with torch.no_grad():\n        outputs = model(images)\n        predictions = torch.argmax(outputs, dim=1)\n\n    # Denormalize images for visualization\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n    images = images * std + mean\n\n    # Create figure with subplots\n    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n\n    for i in range(num_samples):\n        # Display original image\n        axes[i, 0].imshow(images[i].permute(1, 2, 0).cpu().numpy())\n        axes[i, 0].set_title(\"Original Image\")\n        axes[i, 0].axis(\"off\")\n\n        # Display ground truth mask\n        axes[i, 1].imshow(masks[i].cpu().numpy(), cmap=\"gray\")\n        axes[i, 1].set_title(\"Ground Truth\")\n        axes[i, 1].axis(\"off\")\n\n        # Display predicted mask\n        axes[i, 2].imshow(predictions[i].cpu().numpy(), cmap=\"gray\")\n        axes[i, 2].set_title(\"Prediction\")\n        axes[i, 2].axis(\"off\")\n\n    plt.tight_layout()\n    plt.savefig(\"/kaggle/working/mamba_segmentation_with_mkpe_results.png\")\n    plt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T18:55:57.826670Z","iopub.execute_input":"2025-05-28T18:55:57.826969Z","iopub.status.idle":"2025-05-28T18:55:58.018781Z","shell.execute_reply.started":"2025-05-28T18:55:57.826950Z","shell.execute_reply":"2025-05-28T18:55:58.018088Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# -------------------------\n# Main Function - Adapted for Kaggle\n# -------------------------\ndef main():\n    print(\"Starting Mamba-UNet with Multi-Kernel Positional Embedding...\")\n\n    # Set the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Set seeds for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n\n    # Download and set up the dataset\n    polypgen_path = download_and_setup_dataset(force_download=False)\n\n    if not polypgen_path:\n        print(\"Dataset setup failed. Exiting...\")\n        return\n\n    # Define transformations\n    transform = T.Compose([\n        T.Resize((256, 256)),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    target_transform = T.Compose([\n        T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n        T.ToTensor(),\n        lambda x: (x > 0.5).long()\n    ])\n\n    # Create datasets and data loaders\n    try:\n        train_dataset = PolypGenDataset(\n            polypgen_path,\n            split='train',\n            transform=transform,\n            target_transform=target_transform,\n            augment=True\n        )\n\n        val_dataset = PolypGenDataset(\n            polypgen_path,\n            split='val',\n            transform=transform,\n            target_transform=target_transform,\n            augment=False\n        )\n\n        # Use batch size of 4 as requested\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=8,\n            shuffle=True,\n            num_workers=2,\n            pin_memory=True if torch.cuda.is_available() else False\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=8,\n            shuffle=False,\n            num_workers=2,\n            pin_memory=True if torch.cuda.is_available() else False\n        )\n\n        print(\"Data loaders created successfully.\")\n    except Exception as e:\n        print(f\"Error creating datasets: {e}\")\n        import traceback\n        traceback.print_exc()\n        return\n\n    # Initialize model args and create model with MKPE\n    args = ModelArgs()\n    model = MambaUNetWithMKPE(args).to(device)\n    print(\"Mamba-UNet model created with Multi-Kernel Positional Embedding.\")\n\n    # Define enhanced loss function and optimizer\n    criterion = DeepSupervisionLoss(main_weight=0.6, deep2_weight=0.2, deep3_weight=0.2)\n    \n    # Slightly different learning rate for MKPE model\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\n    # Add learning rate scheduler with warm restarts\n    scheduler = CosineAnnealingWarmRestarts(\n        optimizer,\n        T_0=10,  # Restart every 10 epochs\n        T_mult=2,  # Double period after each restart\n        eta_min=1e-6,\n    )\n\n    # Training loop\n    num_epochs = 100\n    best_iou = 0.0\n    patience_counter = 0\n    max_patience = 20  # Early stopping after 20 epochs without improvement\n\n    history = {\n        'train_loss': [], 'train_iou': [], 'train_dice': [],\n        'val_loss': [], 'val_iou': [], 'val_dice': []\n    }\n\n    print(f\"Starting training for {num_epochs} epochs...\")\n\n    try:\n        for epoch in range(num_epochs):\n            print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n            # Train with enhanced functions\n            train_loss, train_iou, train_dice = train_one_epoch_enhanced(\n                model, train_loader, optimizer, criterion, device, scheduler\n            )\n\n            # Validate\n            val_loss, val_iou, val_dice = validate_enhanced(\n                model, val_loader, criterion, device\n            )\n\n            # Save history\n            history['train_loss'].append(train_loss)\n            history['train_iou'].append(train_iou)\n            history['train_dice'].append(train_dice)\n            history['val_loss'].append(val_loss)\n            history['val_iou'].append(val_iou)\n            history['val_dice'].append(val_dice)\n\n            # Print epoch results\n            print(f\"Train - Loss: {train_loss:.4f}, IoU: {train_iou:.4f}, Dice: {train_dice:.4f}\")\n            print(f\"Val   - Loss: {val_loss:.4f}, IoU: {val_iou:.4f}, Dice: {val_dice:.4f}\")\n\n            # Save best model\n            if val_iou > best_iou:\n                best_iou = val_iou\n                torch.save(model.state_dict(), \"/kaggle/working/best_mamba_unet_with_mkpe.pth\")\n                print(f\"Model saved with IoU: {best_iou:.4f}\")\n                patience_counter = 0  # Reset patience counter\n            else:\n                patience_counter += 1\n\n            # Save checkpoint every 10 epochs for safety\n            if (epoch+1) % 10 == 0:\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'best_iou': best_iou,\n                    'history': history,\n                }, f\"/kaggle/working/checkpoint_mkpe_epoch_{epoch+1}.pth\")\n\n                # Plot and save training progress\n                plot_training_progress(history, epoch+1)\n\n            # Early stopping\n            if patience_counter >= max_patience:\n                print(f\"Early stopping after {max_patience} epochs without improvement\")\n                break\n\n            # Clear GPU cache between epochs\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        import traceback\n        traceback.print_exc()\n\n        # Save checkpoint on error\n        torch.save({\n            'epoch': epoch if 'epoch' in locals() else 0,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict() if 'scheduler' in locals() else None,\n            'best_iou': best_iou if 'best_iou' in locals() else 0,\n            'history': history,\n        }, \"/kaggle/working/error_checkpoint_mkpe.pth\")\n\n    # Load best model for evaluation\n    try:\n        model.load_state_dict(torch.load(\"/kaggle/working/best_mamba_unet_with_mkpe.pth\"))\n        print(\"Loaded best model for evaluation\")\n    except:\n        print(\"Could not load best model, using current model\")\n\n    # Visualize results\n    visualize_results(model, val_loader, device)\n\n    print(\"Training and evaluation completed!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T18:55:58.020271Z","iopub.execute_input":"2025-05-28T18:55:58.020529Z"}},"outputs":[{"name":"stdout","text":"Starting Mamba-UNet with Multi-Kernel Positional Embedding...\nUsing device: cuda\nFound PolypGen 'positive' dataset at /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive\nSample image names: ['C1_100H0050.jpg', 'C1_100S0001.jpg', 'C1_100S0003.jpg', 'C1_102OLCV1_100H0006.jpg', 'C1_104OLCV1_100H0002.jpg']\nCreated train dataset with 3009 images\nSample image names: ['C1_100H0050.jpg', 'C1_100S0001.jpg', 'C1_100S0003.jpg', 'C1_102OLCV1_100H0006.jpg', 'C1_104OLCV1_100H0002.jpg']\nCreated val dataset with 376 images\nData loaders created successfully.\nMamba-UNet model created with Multi-Kernel Positional Embedding.\nStarting training for 100 epochs...\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  46%|     | 175/377 [08:05<09:24,  2.79s/it, loss=0.426, iou=0.125, dice=0.125]      ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:28<00:00,  2.78s/it, loss=0.405, iou=0, dice=0]              \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.4377, IoU: 0.1086, Dice: 0.1237\nVal   - Loss: 0.3865, IoU: 0.1462, Dice: 0.1750\nModel saved with IoU: 0.1462\nEpoch 2/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  18%|        | 67/377 [03:07<14:26,  2.80s/it, loss=0.385, iou=0.069, dice=0.108]  ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.206, iou=0.565, dice=0.722]  \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.3439, IoU: 0.1985, Dice: 0.2544\nVal   - Loss: 0.3607, IoU: 0.2242, Dice: 0.2726\nModel saved with IoU: 0.2242\nEpoch 3/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  24%|       | 89/377 [04:09<13:24,  2.79s/it, loss=0.31, iou=0.221, dice=0.267]   ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.466, iou=0.00343, dice=0.00683]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.3220, IoU: 0.2537, Dice: 0.3234\nVal   - Loss: 0.3206, IoU: 0.2656, Dice: 0.3195\nModel saved with IoU: 0.2656\nEpoch 4/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  35%|      | 132/377 [06:09<11:26,  2.80s/it, loss=0.321, iou=0.529, dice=0.609] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:31<00:00,  2.79s/it, loss=0.332, iou=0, dice=0]          \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.3049, IoU: 0.2924, Dice: 0.3668\nVal   - Loss: 0.3215, IoU: 0.2911, Dice: 0.3500\nModel saved with IoU: 0.2911\nEpoch 5/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  44%|     | 166/377 [07:44<09:49,  2.80s/it, loss=0.249, iou=0.388, dice=0.499] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.415, iou=0.291, dice=0.451]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2902, IoU: 0.3400, Dice: 0.4182\nVal   - Loss: 0.2924, IoU: 0.3534, Dice: 0.4210\nModel saved with IoU: 0.3534\nEpoch 6/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  11%|         | 40/377 [01:52<15:41,  2.79s/it, loss=0.251, iou=0.31, dice=0.378] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:31<00:00,  2.79s/it, loss=0.63, iou=0.215, dice=0.353] \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2581, IoU: 0.4250, Dice: 0.5040\nVal   - Loss: 0.2521, IoU: 0.4520, Dice: 0.5290\nModel saved with IoU: 0.4520\nEpoch 7/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  17%|        | 64/377 [02:59<14:35,  2.80s/it, loss=0.184, iou=0.821, dice=0.885]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:31<00:00,  2.79s/it, loss=0.271, iou=0.474, dice=0.644]  \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2440, IoU: 0.4564, Dice: 0.5361\nVal   - Loss: 0.3101, IoU: 0.4091, Dice: 0.4699\nEpoch 8/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  83%| | 312/377 [14:32<03:02,  2.80s/it, loss=0.194, iou=0.578, dice=0.639]   ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:31<00:00,  2.79s/it, loss=0.24, iou=0.277, dice=0.434] \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2553, IoU: 0.4294, Dice: 0.5086\nVal   - Loss: 0.2759, IoU: 0.4389, Dice: 0.4973\nEpoch 9/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  23%|       | 88/377 [04:06<13:25,  2.79s/it, loss=0.321, iou=0.4, dice=0.488]  ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.159, iou=0.478, dice=0.647] \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2399, IoU: 0.4670, Dice: 0.5457\nVal   - Loss: 0.2656, IoU: 0.4706, Dice: 0.5386\nModel saved with IoU: 0.4706\nEpoch 10/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  25%|       | 94/377 [04:23<13:12,  2.80s/it, loss=0.328, iou=0.498, dice=0.607]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.425, iou=0.0705, dice=0.132]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2204, IoU: 0.5043, Dice: 0.5818\nVal   - Loss: 0.2350, IoU: 0.4666, Dice: 0.5509\nEpoch 11/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  69%|   | 262/377 [12:13<05:23,  2.81s/it, loss=0.199, iou=0.63, dice=0.718]  ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.15, iou=0.484, dice=0.652]  \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2036, IoU: 0.5432, Dice: 0.6192\nVal   - Loss: 0.2117, IoU: 0.5676, Dice: 0.6347\nModel saved with IoU: 0.5676\nEpoch 12/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  56%|    | 213/377 [09:56<07:38,  2.80s/it, loss=0.135, iou=0.893, dice=0.935] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.0846, iou=0.722, dice=0.839]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1901, IoU: 0.5763, Dice: 0.6507\nVal   - Loss: 0.2034, IoU: 0.5893, Dice: 0.6567\nModel saved with IoU: 0.5893\nEpoch 13/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  68%|   | 257/377 [11:59<05:35,  2.79s/it, loss=0.162, iou=0.587, dice=0.637] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.229, iou=0.214, dice=0.353] \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1818, IoU: 0.5934, Dice: 0.6661\nVal   - Loss: 0.1862, IoU: 0.5905, Dice: 0.6623\nModel saved with IoU: 0.5905\nEpoch 14/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|         | 13/377 [00:36<17:01,  2.81s/it, loss=0.262, iou=0.423, dice=0.491]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.048, iou=0.869, dice=0.93]   \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1938, IoU: 0.5714, Dice: 0.6457\nVal   - Loss: 0.2233, IoU: 0.5035, Dice: 0.5795\nEpoch 15/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:   3%|         | 13/377 [00:37<17:02,  2.81s/it, loss=0.22, iou=0.539, dice=0.614] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.153, iou=0.519, dice=0.683] \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2097, IoU: 0.5355, Dice: 0.6118\nVal   - Loss: 0.2236, IoU: 0.5477, Dice: 0.6184\nEpoch 16/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  95%|| 359/377 [16:46<00:50,  2.80s/it, loss=0.144, iou=0.696, dice=0.804] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:34<00:00,  2.80s/it, loss=0.312, iou=0, dice=0]        \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.2028, IoU: 0.5532, Dice: 0.6283\nVal   - Loss: 0.2269, IoU: 0.5265, Dice: 0.6036\nEpoch 17/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  43%|     | 162/377 [07:34<10:03,  2.81s/it, loss=0.117, iou=0.641, dice=0.724] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.0803, iou=0.816, dice=0.898]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1908, IoU: 0.5812, Dice: 0.6543\nVal   - Loss: 0.2136, IoU: 0.5399, Dice: 0.6210\nEpoch 18/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  31%|       | 116/377 [05:25<12:10,  2.80s/it, loss=0.15, iou=0.598, dice=0.657]  ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:34<00:00,  2.80s/it, loss=0.276, iou=0, dice=0]         \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1837, IoU: 0.5963, Dice: 0.6690\nVal   - Loss: 0.1852, IoU: 0.6149, Dice: 0.6826\nModel saved with IoU: 0.6149\nEpoch 19/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  66%|   | 248/377 [11:34<05:59,  2.79s/it, loss=0.204, iou=0.4, dice=0.465]   ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.0389, iou=0.897, dice=0.946]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1756, IoU: 0.6139, Dice: 0.6853\nVal   - Loss: 0.1862, IoU: 0.6291, Dice: 0.6954\nModel saved with IoU: 0.6291\nEpoch 20/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  32%|      | 121/377 [05:38<11:57,  2.80s/it, loss=0.17, iou=0.584, dice=0.649]  ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.3, iou=0.636, dice=0.778]   \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1670, IoU: 0.6282, Dice: 0.6973\nVal   - Loss: 0.1732, IoU: 0.6614, Dice: 0.7283\nModel saved with IoU: 0.6614\nEpoch 21/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  16%|        | 62/377 [02:54<14:44,  2.81s/it, loss=0.604, iou=0.439, dice=0.513] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.0207, iou=0.946, dice=0.972]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1617, IoU: 0.6530, Dice: 0.7206\nVal   - Loss: 0.1645, IoU: 0.6596, Dice: 0.7223\nEpoch 22/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  82%| | 308/377 [14:22<03:12,  2.79s/it, loss=0.136, iou=0.674, dice=0.729] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.0983, iou=0.606, dice=0.755]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1528, IoU: 0.6621, Dice: 0.7285\nVal   - Loss: 0.1631, IoU: 0.6838, Dice: 0.7458\nModel saved with IoU: 0.6838\nEpoch 23/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:   4%|         | 14/377 [00:39<16:58,  2.80s/it, loss=0.139, iou=0.622, dice=0.666]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.484, iou=0.436, dice=0.607] \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1485, IoU: 0.6767, Dice: 0.7428\nVal   - Loss: 0.1699, IoU: 0.6836, Dice: 0.7472\nEpoch 24/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  37%|      | 138/377 [06:26<11:08,  2.80s/it, loss=0.129, iou=0.745, dice=0.821] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.2, iou=0.512, dice=0.677]   \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1412, IoU: 0.6912, Dice: 0.7550\nVal   - Loss: 0.1635, IoU: 0.6840, Dice: 0.7445\nModel saved with IoU: 0.6840\nEpoch 25/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  75%|  | 283/377 [13:12<04:24,  2.81s/it, loss=0.172, iou=0.591, dice=0.67]  ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.80s/it, loss=0.253, iou=1, dice=1]         \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1383, IoU: 0.6989, Dice: 0.7619\nVal   - Loss: 0.1507, IoU: 0.7001, Dice: 0.7609\nModel saved with IoU: 0.7001\nEpoch 26/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  20%|        | 77/377 [03:36<13:57,  2.79s/it, loss=0.0865, iou=0.777, dice=0.822]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:32<00:00,  2.79s/it, loss=0.0128, iou=0.978, dice=0.989]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:31<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1348, IoU: 0.7052, Dice: 0.7675\nVal   - Loss: 0.1509, IoU: 0.7123, Dice: 0.7716\nModel saved with IoU: 0.7123\nEpoch 27/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  55%|    | 209/377 [09:45<07:50,  2.80s/it, loss=0.19, iou=0.585, dice=0.642]  ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:34<00:00,  2.80s/it, loss=0.0721, iou=0.718, dice=0.836]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1347, IoU: 0.7028, Dice: 0.7652\nVal   - Loss: 0.1498, IoU: 0.7103, Dice: 0.7707\nEpoch 28/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  23%|       | 87/377 [04:03<13:30,  2.80s/it, loss=0.161, iou=0.781, dice=0.841] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.322, iou=0, dice=0]         \nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1628, IoU: 0.6388, Dice: 0.7067\nVal   - Loss: 0.1842, IoU: 0.6400, Dice: 0.7050\nEpoch 29/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  31%|       | 117/377 [05:27<12:07,  2.80s/it, loss=0.109, iou=0.785, dice=0.859]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 377/377 [17:33<00:00,  2.79s/it, loss=0.0557, iou=0.825, dice=0.904]\nValidation:   0%|          | 0/47 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/C3_C3_EndoCV2021_00429.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/C3_C3_EndoCV2021_00429.jpg\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 47/47 [00:32<00:00,  1.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train - Loss: 0.1671, IoU: 0.6321, Dice: 0.7008\nVal   - Loss: 0.1726, IoU: 0.6567, Dice: 0.7229\nEpoch 30/100\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/377 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training batch - Images: torch.Size([8, 3, 256, 256]), Masks: torch.Size([8, 256, 256])\nMasks unique values: tensor([0, 1], device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"Training:  11%|         | 41/377 [01:55<15:42,  2.80s/it, loss=0.101, iou=0.688, dice=0.761] ","output_type":"stream"},{"name":"stdout","text":"Image path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/images/seq10_2_endocv2021_positive_942.jpg\nMask path: /kaggle/input/polypgen2021/PolypGen2021_MultiCenterData_v3/positive/masks/seq10_2_endocv2021_positive_942.jpg\n","output_type":"stream"},{"name":"stderr","text":"Training:  42%|     | 160/377 [07:28<10:06,  2.79s/it, loss=0.182, iou=0.772, dice=0.817] ","output_type":"stream"}],"execution_count":null}]}